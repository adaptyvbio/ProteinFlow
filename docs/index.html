<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>proteinflow API documentation</title>
<meta name="description" content="`proteinflow` is a pipeline that loads protein data from PDB, filters it, puts it in a machine readable format and extracts structure and sequence â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.png?">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>proteinflow</code></h1>
</header>
<section id="section-intro">
<p><code><a title="proteinflow" href="#proteinflow">proteinflow</a></code> is a pipeline that loads protein data from PDB, filters it, puts it in a machine readable format and extracts structure and sequence features. </p>
<p><img alt="pipeline" src="https://raw.githubusercontent.com/adaptyvbio/ProteinFlow/main/media/fig_pipeline.png"></p>
<h2 id="installation">Installation</h2>
<p>Recommended: create a new <code>conda</code> environment and install <code><a title="proteinflow" href="#proteinflow">proteinflow</a></code> and <code>mmseqs</code>. Note that the python version has to be between 3.8 and 3.10. </p>
<pre><code class="language-bash">conda create --name proteinflow -y python=3.9
conda activate proteinflow
conda install -y -c conda-forge -c bioconda mmseqs2
python -m pip install proteinflow
</code></pre>
<p>In addition, <code><a title="proteinflow" href="#proteinflow">proteinflow</a></code> depends on the <code>rcsbsearch</code> package and the latest release is currently failing. Follow the recommended fix:</p>
<pre><code class="language-bash">python -m pip install &quot;rcsbsearch @ git+https://github.com/sbliven/rcsbsearch@dbdfe3880cc88b0ce57163987db613d579400c8e&quot;
</code></pre>
<p>Note that you do not need to install <code>mmseqs</code> or <code>rcsbsearch</code> if you are not planning to generate a new dataset.</p>
<p>Finally, you can use our <a href="https://hub.docker.com/r/adaptyvbio/proteinflow/tags">docker image</a> as an alternative.</p>
<h2 id="usage">Usage</h2>
<h3 id="downloading-pre-computed-datasets">Downloading pre-computed datasets</h3>
<p>We have already run the pipeline with a consensus set of parameters and saved the results at a server. You can download the resulting dataset with <code><a title="proteinflow" href="#proteinflow">proteinflow</a></code>. Check the output of <code><a title="proteinflow" href="#proteinflow">proteinflow</a> check_tags</code> for a list of available tags.</p>
<pre><code class="language-bash">proteinflow download --tag paper 
</code></pre>
<p>See <code><a title="proteinflow.download_data" href="#proteinflow.download_data">download_data()</a></code> (or run <code>proteinflow download --help</code>) for more information.</p>
<h3 id="running-the-pipeline">Running the pipeline</h3>
<p>You can also run <code><a title="proteinflow" href="#proteinflow">proteinflow</a></code> with your own parameters. Check the output of <code><a title="proteinflow" href="#proteinflow">proteinflow</a> check_snapshots</code> for a list of available snapshots (naming rule: <code>{year}{month}{day}</code>).</p>
<p>For instance, let's generate a dataset with the following description:
- resolution threshold 5 $\AA$,
- PDB snapshot 20190101,
- all structure methods accepted,
- sequence identity threshold for clustering 40%,
- maximum length per sequence 1000 residues,
- minimum length per sequence 5 residues,
- maximum fraction of missing values at the ends 10%,
- validation subset 10%.</p>
<pre><code class="language-bash">proteinflow generate --tag new --resolution_thr 5 --pdb_snapshot 20190101 --not_filter_methods --min_seq_id 0.4 --max_length 1000 --min_length 5 --missing_ends_thr 0.1 --valid_split 0.1
</code></pre>
<p>See <code><a title="proteinflow.generate_data" href="#proteinflow.generate_data">generate_data()</a></code> (or run <code>proteinflow generate --help</code>) for the full list of parameters and more information.</p>
<p>The reasons for filtering files out are logged in text files (at <code>data/logs</code> by default). To get a summary, run <code>proteinflow get_summary {log_path}</code>.</p>
<h3 id="splitting">Splitting</h3>
<p>By default, both <code><a title="proteinflow" href="#proteinflow">proteinflow</a> generate</code> and <code><a title="proteinflow" href="#proteinflow">proteinflow</a> download</code> will also split your data into training, test and validation according to MMseqs2 clustering and homomer/heteromer/single chain proportions. However, you can skip this step with a <code>--skip_splitting</code> flag and then perform it separately with the <code><a title="proteinflow" href="#proteinflow">proteinflow</a> split</code> command.</p>
<p>The following command will perform the splitting with a 10% validation set, a 5% test set and a 50% threshold for sequence identity clusters.</p>
<pre><code class="language-bash">proteinflow split --tag new --valid_split 0.1 --test_split 0.5 --min_seq_id 0.5
</code></pre>
<p>See <code><a title="proteinflow.split_data" href="#proteinflow.split_data">split_data()</a></code> (or run <code>proteinflow split --help</code>) for more information.</p>
<h3 id="using-the-data">Using the data</h3>
<p>The output files are pickled nested dictionaries where first-level keys are chain Ids and second-level keys are the following:
- <code>'crd_bb'</code>: a <code>numpy</code> array of shape <code>(L, 4, 3)</code> with backbone atom coordinates (N, C, CA, O),
- <code>'crd_sc'</code>: a <code>numpy</code> array of shape <code>(L, 10, 3)</code> with sidechain atom coordinates (check <code><a title="proteinflow.sidechain_order" href="#proteinflow.sidechain_order">sidechain_order()</a></code> for the order of atoms),
- <code>'msk'</code>: a <code>numpy</code> array of shape <code>(L,)</code> where ones correspond to residues with known coordinates and
zeros to missing values,
- <code>'seq'</code>: a string of length <code>L</code> with residue types.</p>
<p>Once your data is ready, you can open the files directly with <code>pickle</code> to access this data.</p>
<pre><code class="language-python">import pickle
import os

train_folder = &quot;./data/proteinflow_new/training&quot;
for filename in os.listdir(train_folder):
    with open(os.path.join(train_folder, filename), &quot;rb&quot;) as f:
        data = pickle.load(f)
    crd_bb = data[&quot;crd_bb&quot;]
    seq = data[&quot;seq&quot;]
    ...
</code></pre>
<p>Alternatively, you can use our <code><a title="proteinflow.ProteinDataset" href="#proteinflow.ProteinDataset">ProteinDataset</a></code> or <code><a title="proteinflow.ProteinLoader" href="#proteinflow.ProteinLoader">ProteinLoader</a></code> classes
for convenient processing. Among other things, they allow for feature extraction, single chain / homomer / heteromer filtering and randomized sampling from sequence identity clusters.</p>
<p>For example, here is how we can create a data loader that:
- samples a different cluster representative at every epoch,
- extracts dihedral angles, sidechain orientation and secondary structure features,
- only loads pairs of interacting proteins (larger biounits are broken up into pairs),
- generates a geometric mask for 10% of one of the chains (random generation at every pass),
- has batch size 8.</p>
<pre><code class="language-python">from proteinflow import ProteinLoader
train_loader = ProteinLoader.from_args(
    &quot;./data/proteinflow_new/training&quot;, 
    clustering_dict_path=&quot;./data/proteinflow_new/splits_dict/train.pickle&quot;,
    node_features_type=&quot;dihedral+sidechain_orientation+secondary_structure&quot;,
    entry_type=&quot;pair&quot;,
    mask_frac=0.1,
    batch_size=8,
)
for batch in train_loader:
    crd_bb = batch[&quot;X&quot;] #(B, L, 4, 3)
    seq = batch[&quot;S&quot;] #(B, L)
    sse = batch[&quot;secondary_structure&quot;] #(B, L, 3)
    to_predict = batch[&quot;masked_res&quot;] #(B, L), 1 where the residues should be masked, 0 otherwise
    ...
</code></pre>
<p>See <code><a title="proteinflow.ProteinLoader" href="#proteinflow.ProteinLoader">ProteinLoader</a></code> for more information.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
`proteinflow` is a pipeline that loads protein data from PDB, filters it, puts it in a machine readable format and extracts structure and sequence features. 

![pipeline](https://raw.githubusercontent.com/adaptyvbio/ProteinFlow/main/media/fig_pipeline.png)

## Installation
Recommended: create a new `conda` environment and install `proteinflow` and `mmseqs`. Note that the python version has to be between 3.8 and 3.10. 
```bash
conda create --name proteinflow -y python=3.9
conda activate proteinflow
conda install -y -c conda-forge -c bioconda mmseqs2
python -m pip install proteinflow
```
In addition, `proteinflow` depends on the `rcsbsearch` package and the latest release is currently failing. Follow the recommended fix:
```bash
python -m pip install &#34;rcsbsearch @ git+https://github.com/sbliven/rcsbsearch@dbdfe3880cc88b0ce57163987db613d579400c8e&#34;
```

Note that you do not need to install `mmseqs` or `rcsbsearch` if you are not planning to generate a new dataset.

Finally, you can use our [docker image](https://hub.docker.com/r/adaptyvbio/proteinflow/tags) as an alternative.

## Usage
### Downloading pre-computed datasets
We have already run the pipeline with a consensus set of parameters and saved the results at a server. You can download the resulting dataset with `proteinflow`. Check the output of `proteinflow check_tags` for a list of available tags.
```bash
proteinflow download --tag paper 
```

See `proteinflow.download_data` (or run `proteinflow download --help`) for more information.

### Running the pipeline
You can also run `proteinflow` with your own parameters. Check the output of `proteinflow check_snapshots` for a list of available snapshots (naming rule: `{year}{month}{day}`).

For instance, let&#39;s generate a dataset with the following description:
- resolution threshold 5 $\AA$,
- PDB snapshot 20190101,
- all structure methods accepted,
- sequence identity threshold for clustering 40%,
- maximum length per sequence 1000 residues,
- minimum length per sequence 5 residues,
- maximum fraction of missing values at the ends 10%,
- validation subset 10%.

```bash
proteinflow generate --tag new --resolution_thr 5 --pdb_snapshot 20190101 --not_filter_methods --min_seq_id 0.4 --max_length 1000 --min_length 5 --missing_ends_thr 0.1 --valid_split 0.1
```
See `proteinflow.generate_data` (or run `proteinflow generate --help`) for the full list of parameters and more information.

The reasons for filtering files out are logged in text files (at `data/logs` by default). To get a summary, run `proteinflow get_summary {log_path}`.

### Splitting
By default, both `proteinflow generate` and `proteinflow download` will also split your data into training, test and validation according to MMseqs2 clustering and homomer/heteromer/single chain proportions. However, you can skip this step with a `--skip_splitting` flag and then perform it separately with the `proteinflow split` command.

The following command will perform the splitting with a 10% validation set, a 5% test set and a 50% threshold for sequence identity clusters.
```bash
proteinflow split --tag new --valid_split 0.1 --test_split 0.5 --min_seq_id 0.5
```

See `proteinflow.split_data` (or run `proteinflow split --help`) for more information.

### Using the data
The output files are pickled nested dictionaries where first-level keys are chain Ids and second-level keys are the following:
- `&#39;crd_bb&#39;`: a `numpy` array of shape `(L, 4, 3)` with backbone atom coordinates (N, C, CA, O),
- `&#39;crd_sc&#39;`: a `numpy` array of shape `(L, 10, 3)` with sidechain atom coordinates (check `proteinflow.sidechain_order()` for the order of atoms),
- `&#39;msk&#39;`: a `numpy` array of shape `(L,)` where ones correspond to residues with known coordinates and
    zeros to missing values,
- `&#39;seq&#39;`: a string of length `L` with residue types.

Once your data is ready, you can open the files directly with `pickle` to access this data.

```python
import pickle
import os

train_folder = &#34;./data/proteinflow_new/training&#34;
for filename in os.listdir(train_folder):
    with open(os.path.join(train_folder, filename), &#34;rb&#34;) as f:
        data = pickle.load(f)
    crd_bb = data[&#34;crd_bb&#34;]
    seq = data[&#34;seq&#34;]
    ...
```

Alternatively, you can use our `ProteinDataset` or `ProteinLoader` classes 
for convenient processing. Among other things, they allow for feature extraction, single chain / homomer / heteromer filtering and randomized sampling from sequence identity clusters.

For example, here is how we can create a data loader that:
- samples a different cluster representative at every epoch,
- extracts dihedral angles, sidechain orientation and secondary structure features,
- only loads pairs of interacting proteins (larger biounits are broken up into pairs),
- generates a geometric mask for 10% of one of the chains (random generation at every pass),
- has batch size 8.

```python
from proteinflow import ProteinLoader
train_loader = ProteinLoader.from_args(
    &#34;./data/proteinflow_new/training&#34;, 
    clustering_dict_path=&#34;./data/proteinflow_new/splits_dict/train.pickle&#34;,
    node_features_type=&#34;dihedral+sidechain_orientation+secondary_structure&#34;,
    entry_type=&#34;pair&#34;,
    mask_frac=0.1,
    batch_size=8,
)
for batch in train_loader:
    crd_bb = batch[&#34;X&#34;] #(B, L, 4, 3)
    seq = batch[&#34;S&#34;] #(B, L)
    sse = batch[&#34;secondary_structure&#34;] #(B, L, 3)
    to_predict = batch[&#34;masked_res&#34;] #(B, L), 1 where the residues should be masked, 0 otherwise
    ...
```

See `proteinflow.ProteinLoader` for more information.
&#34;&#34;&#34;

__pdoc__ = {&#34;utils&#34;: False, &#34;scripts&#34;: False}
__docformat__ = &#34;numpy&#34;

from proteinflow.utils.filter_database import _remove_database_redundancies
from proteinflow.utils.process_pdb import (
    _align_structure,
    _open_structure,
    PDBError,
    _s3list,
    SIDECHAIN_ORDER,
)
from proteinflow.utils.cluster_and_partition import (
    _build_dataset_partition,
    _check_mmseqs,
)
from proteinflow.utils.split_dataset import _download_dataset, _split_data
from proteinflow.utils.biotite_sse import _annotate_sse
from proteinflow.utils.async_download import _download_s3_parallel

from aiobotocore.session import get_session
import traceback
import shutil
import warnings
import os
import pickle
from collections import defaultdict
from rcsbsearch import Attr
from datetime import datetime
import subprocess
import urllib
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import random
import os
import pickle
from p_tqdm import p_map
from collections import defaultdict
from tqdm import tqdm
from copy import deepcopy
import pandas as pd
from numpy import linalg
import boto3
from botocore import UNSIGNED
from botocore.config import Config
from concurrent import futures
from concurrent.futures import ThreadPoolExecutor
from itertools import combinations
import requests


MAIN_ATOMS = {
    &#34;GLY&#34;: None,
    &#34;ALA&#34;: 0,
    &#34;VAL&#34;: 0,
    &#34;LEU&#34;: 1,
    &#34;ILE&#34;: 1,
    &#34;MET&#34;: 2,
    &#34;PRO&#34;: 1,
    &#34;TRP&#34;: 5,
    &#34;PHE&#34;: 6,
    &#34;TYR&#34;: 7,
    &#34;CYS&#34;: 1,
    &#34;SER&#34;: 1,
    &#34;THR&#34;: 1,
    &#34;ASN&#34;: 1,
    &#34;GLN&#34;: 2,
    &#34;HIS&#34;: 2,
    &#34;LYS&#34;: 3,
    &#34;ARG&#34;: 4,
    &#34;ASP&#34;: 1,
    &#34;GLU&#34;: 2,
}
D3TO1 = {
    &#34;CYS&#34;: &#34;C&#34;,
    &#34;ASP&#34;: &#34;D&#34;,
    &#34;SER&#34;: &#34;S&#34;,
    &#34;GLN&#34;: &#34;Q&#34;,
    &#34;LYS&#34;: &#34;K&#34;,
    &#34;ILE&#34;: &#34;I&#34;,
    &#34;PRO&#34;: &#34;P&#34;,
    &#34;THR&#34;: &#34;T&#34;,
    &#34;PHE&#34;: &#34;F&#34;,
    &#34;ASN&#34;: &#34;N&#34;,
    &#34;GLY&#34;: &#34;G&#34;,
    &#34;HIS&#34;: &#34;H&#34;,
    &#34;LEU&#34;: &#34;L&#34;,
    &#34;ARG&#34;: &#34;R&#34;,
    &#34;TRP&#34;: &#34;W&#34;,
    &#34;ALA&#34;: &#34;A&#34;,
    &#34;VAL&#34;: &#34;V&#34;,
    &#34;GLU&#34;: &#34;E&#34;,
    &#34;TYR&#34;: &#34;Y&#34;,
    &#34;MET&#34;: &#34;M&#34;,
}
ALPHABET = &#34;-ACDEFGHIKLMNPQRSTVWY&#34;

FEATURES_DICT = defaultdict(lambda: defaultdict(lambda: 0))
FEATURES_DICT[&#34;hydropathy&#34;].update(
    {
        &#34;-&#34;: 0,
        &#34;I&#34;: 4.5,
        &#34;V&#34;: 4.2,
        &#34;L&#34;: 3.8,
        &#34;F&#34;: 2.8,
        &#34;C&#34;: 2.5,
        &#34;M&#34;: 1.9,
        &#34;A&#34;: 1.8,
        &#34;W&#34;: -0.9,
        &#34;G&#34;: -0.4,
        &#34;T&#34;: -0.7,
        &#34;S&#34;: -0.8,
        &#34;Y&#34;: -1.3,
        &#34;P&#34;: -1.6,
        &#34;H&#34;: -3.2,
        &#34;N&#34;: -3.5,
        &#34;D&#34;: -3.5,
        &#34;Q&#34;: -3.5,
        &#34;E&#34;: -3.5,
        &#34;K&#34;: -3.9,
        &#34;R&#34;: -4.5,
    }
)
FEATURES_DICT[&#34;volume&#34;].update(
    {
        &#34;-&#34;: 0,
        &#34;G&#34;: 60.1,
        &#34;A&#34;: 88.6,
        &#34;S&#34;: 89.0,
        &#34;C&#34;: 108.5,
        &#34;D&#34;: 111.1,
        &#34;P&#34;: 112.7,
        &#34;N&#34;: 114.1,
        &#34;T&#34;: 116.1,
        &#34;E&#34;: 138.4,
        &#34;V&#34;: 140.0,
        &#34;Q&#34;: 143.8,
        &#34;H&#34;: 153.2,
        &#34;M&#34;: 162.9,
        &#34;I&#34;: 166.7,
        &#34;L&#34;: 166.7,
        &#34;K&#34;: 168.6,
        &#34;R&#34;: 173.4,
        &#34;F&#34;: 189.9,
        &#34;Y&#34;: 193.6,
        &#34;W&#34;: 227.8,
    }
)
FEATURES_DICT[&#34;charge&#34;].update(
    {
        **{&#34;R&#34;: 1, &#34;K&#34;: 1, &#34;D&#34;: -1, &#34;E&#34;: -1, &#34;H&#34;: 0.1},
        **{x: 0 for x in &#34;ABCFGIJLMNOPQSTUVWXYZ-&#34;},
    }
)
FEATURES_DICT[&#34;polarity&#34;].update(
    {**{x: 1 for x in &#34;RNDQEHKSTY&#34;}, **{x: 0 for x in &#34;ACGILMFPWV-&#34;}}
)
FEATURES_DICT[&#34;acceptor&#34;].update(
    {**{x: 1 for x in &#34;DENQHSTY&#34;}, **{x: 0 for x in &#34;RKWACGILMFPV-&#34;}}
)
FEATURES_DICT[&#34;donor&#34;].update(
    {**{x: 1 for x in &#34;RKWNQHSTY&#34;}, **{x: 0 for x in &#34;DEACGILMFPV-&#34;}}
)
_PMAP = lambda x: [
    FEATURES_DICT[&#34;hydropathy&#34;][x] / 5,
    FEATURES_DICT[&#34;volume&#34;][x] / 200,
    FEATURES_DICT[&#34;charge&#34;][x],
    FEATURES_DICT[&#34;polarity&#34;][x],
    FEATURES_DICT[&#34;acceptor&#34;][x],
    FEATURES_DICT[&#34;donor&#34;][x],
]


def _clean(pdb_id, tmp_folder):
    &#34;&#34;&#34;
    Remove all temporary files associated with a PDB ID
    &#34;&#34;&#34;
    for file in os.listdir(tmp_folder):
        if file.startswith(f&#34;{pdb_id}.&#34;):
            subprocess.run(
                [&#34;rm&#34;, os.path.join(tmp_folder, file)],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )


def _log_exception(exception, log_file, pdb_id, tmp_folder):
    &#34;&#34;&#34;
    Record the error in the log file
    &#34;&#34;&#34;

    _clean(pdb_id, tmp_folder)
    if isinstance(exception, PDBError):
        with open(log_file, &#34;a&#34;) as f:
            f.write(f&#34;&lt;&lt;&lt; {str(exception)}: {pdb_id} \n&#34;)
    else:
        with open(log_file, &#34;a&#34;) as f:
            f.write(f&#34;&lt;&lt;&lt; Unknown: {pdb_id} \n&#34;)
            f.write(traceback.format_exc())
            f.write(&#34;\n&#34;)


def _log_removed(removed, log_file):
    &#34;&#34;&#34;
    Record which files we removed due to redundancy
    &#34;&#34;&#34;

    for pdb_id in removed:
        with open(log_file, &#34;a&#34;) as f:
            f.write(f&#34;&lt;&lt;&lt; Removed due to redundancy: {pdb_id} \n&#34;)


def _get_split_dictionaries(
    tmp_folder=&#34;./data/tmp_pdb&#34;,
    output_folder=&#34;./data/pdb&#34;,
    split_tolerance=0.2,
    test_split=0.05,
    valid_split=0.05,
    out_split_dict_folder=&#34;./data/dataset_splits_dict&#34;,
    min_seq_id=0.3,
):
    &#34;&#34;&#34;
    Split preprocessed data into training, validation and test

    Parameters
    ----------
    tmp_folder : str, default &#34;./data/tmp_pdb&#34;
        The folder where temporary files will be saved
    output_folder : str, default &#34;./data/pdb&#34;
        The folder where the output files will be saved
    split_tolerance : float, default 0.2
        The tolerance on the split ratio (default 20%)
    test_split : float, default 0.05
        The percentage of chains to put in the test set (default 5%)
    valid_split : float, default 0.05
        The percentage of chains to put in the validation set (default 5%)
    out_split_dict_folder : str, default &#34;./data/dataset_splits_dict&#34;
        The folder where the dictionaries containing the train/validation/test splits information will be saved&#34;
    min_seq_id : float in [0, 1], default 0.3
        minimum sequence identity for `mmseqs`
    &#34;&#34;&#34;

    os.makedirs(out_split_dict_folder, exist_ok=True)
    (
        train_clusters_dict,
        train_classes_dict,
        valid_clusters_dict,
        valid_classes_dict,
        test_clusters_dict,
        test_classes_dict,
    ) = _build_dataset_partition(
        output_folder,
        tmp_folder,
        valid_split=valid_split,
        test_split=test_split,
        tolerance=split_tolerance,
        min_seq_id=min_seq_id,
    )
    with open(os.path.join(out_split_dict_folder, &#34;train.pickle&#34;), &#34;wb&#34;) as f:
        pickle.dump(train_clusters_dict, f)
        pickle.dump(train_classes_dict, f)
    with open(os.path.join(out_split_dict_folder, &#34;valid.pickle&#34;), &#34;wb&#34;) as f:
        pickle.dump(valid_clusters_dict, f)
        pickle.dump(valid_classes_dict, f)
    with open(os.path.join(out_split_dict_folder, &#34;test.pickle&#34;), &#34;wb&#34;) as f:
        pickle.dump(test_clusters_dict, f)
        pickle.dump(test_classes_dict, f)


def _raise_rcsbsearch(e):
    if &#34;404 Client Error&#34; in str(e):
        raise RuntimeError(
            &#39;Quering rcsbsearch is failing. Please install a version of rcsbsearch where this error is solved:\npython -m pip install &#34;rcsbsearch @ git+https://github.com/sbliven/rcsbsearch@dbdfe3880cc88b0ce57163987db613d579400c8e&#34;&#39;
        )
    else:
        raise e


def _run_processing(
    tmp_folder=&#34;./data/tmp_pdb&#34;,
    output_folder=&#34;./data/pdb&#34;,
    log_folder=&#34;./data/logs&#34;,
    min_length=30,
    max_length=10000,
    resolution_thr=3.5,
    missing_ends_thr=0.3,
    missing_middle_thr=0.1,
    filter_methods=True,
    remove_redundancies=False,
    seq_identity_threshold=0.9,
    n=None,
    force=False,
    tag=None,
    pdb_snapshot=None,
    load_live=False,
):
    &#34;&#34;&#34;
    Download and parse PDB files that meet filtering criteria

    The output files are pickled nested dictionaries where first-level keys are chain Ids and second-level keys are
    the following:

    - `&#39;crd_bb&#39;`: a `numpy` array of shape `(L, 4, 3)` with backbone atom coordinates (N, C, CA, O),
    - `&#39;crd_sc&#39;`: a `numpy` array of shape `(L, 10, 3)` with sidechain atom coordinates (in a fixed order, check `sidechain_order()`),
    - `&#39;msk&#39;`: a `numpy` array of shape `(L,)` where ones correspond to residues with known coordinates and
        zeros to missing values,
    - `&#39;seq&#39;`: a string of length `L` with residue types.

    All errors including reasons for filtering a file out are logged in a log file.

    Parameters
    ----------
    tmp_folder : str, default &#34;./data/tmp_pdb&#34;
        The folder where temporary files will be saved
    output_folder : str, default &#34;./data/pdb&#34;
        The folder where the output files will be saved
    log_folder : str, default &#34;./data/logs&#34;
        The folder where the log file will be saved
    min_length : int, default 30
        The minimum number of non-missing residues per chain
    max_length : int, default 10000
        The maximum number of residues per chain (set None for no threshold)
    resolution_thr : float, default 3.5
        The maximum resolution
    missing_ends_thr : float, default 0.3
        The maximum fraction of missing residues at the ends
    missing_middle_thr : float, default 0.1
        The maximum fraction of missing residues in the middle (after missing ends are disregarded)
    filter_methods : bool, default True
        If `True`, only files obtained with X-ray or EM will be processed
    remove_redundancies : bool, default False
        If `True`, removes biounits that are doubles of others sequence wise
    seq_identity_threshold : float, default 0.9
        The threshold upon which sequences are considered as one and the same (default: 90%)
    n : int, default None
        The number of files to process (for debugging purposes)
    force : bool, default False
        When `True`, rewrite the files if they already exist
    split_tolerance : float, default 0.2
        The tolerance on the split ratio (default 20%)
    split_database : bool, default False
        Whether or not to split the database
    test_split : float, default 0.05
        The percentage of chains to put in the test set (default 5%)
    valid_split : float, default 0.05
        The percentage of chains to put in the validation set (default 5%)
    out_split_dict_folder : str, default &#34;./data/dataset_splits_dict&#34;
        The folder where the dictionaries containing the train/validation/test splits information will be saved
    tag : str, optional
        A tag to add to the log file
    pdb_snapshot : str, optional
        the PDB snapshot to use, by default the latest is used
    load_live : bool, default False
        if `True`, load the files that are not in the latest PDB snapshot from the PDB FTP server (forced to `False` if `pdb_snapshot` is not `None`)

    Returns
    -------
    log : dict
        a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors
    &#34;&#34;&#34;

    TMP_FOLDER = tmp_folder
    OUTPUT_FOLDER = output_folder
    MIN_LENGTH = min_length
    MAX_LENGTH = max_length
    RESOLUTION_THR = resolution_thr
    MISSING_ENDS_THR = missing_ends_thr
    MISSING_MIDDLE_THR = missing_middle_thr

    if not os.path.exists(TMP_FOLDER):
        os.mkdir(TMP_FOLDER)
    if not os.path.exists(OUTPUT_FOLDER):
        os.mkdir(OUTPUT_FOLDER)
    if not os.path.exists(log_folder):
        os.mkdir(log_folder)

    if pdb_snapshot is not None:
        load_live = False

    i = 0
    while os.path.exists(os.path.join(log_folder, f&#34;log_{i}.txt&#34;)):
        i += 1
    LOG_FILE = os.path.join(log_folder, f&#34;log_{i}.txt&#34;)
    print(f&#34;Log file: {LOG_FILE} \n&#34;)
    now = datetime.now()  # current date and time
    date_time = now.strftime(&#34;%m/%d/%Y, %H:%M:%S&#34;) + &#34;\n\n&#34;
    with open(LOG_FILE, &#34;a&#34;) as f:
        f.write(date_time)
        if tag is not None:
            f.write(f&#34;tag: {tag} \n\n&#34;)

    # get filtered PDB ids from PDB API
    pdb_ids = (
        Attr(&#34;rcsb_entry_info.selected_polymer_entity_types&#34;)
        .__eq__(&#34;Protein (only)&#34;)
        .or_(&#34;rcsb_entry_info.polymer_composition&#34;)
        .__eq__(&#34;protein/oligosaccharide&#34;)
    )
    # if include_na:
    #     pdb_ids = pdb_ids.or_(&#39;rcsb_entry_info.polymer_composition&#39;).in_([&#34;protein/NA&#34;, &#34;protein/NA/oligosaccharide&#34;])

    if RESOLUTION_THR is not None:
        pdb_ids = pdb_ids.and_(&#34;rcsb_entry_info.resolution_combined&#34;).__le__(
            RESOLUTION_THR
        )
    if filter_methods:
        pdb_ids = pdb_ids.and_(&#34;exptl.method&#34;).in_(
            [&#34;X-RAY DIFFRACTION&#34;, &#34;ELECTRON MICROSCOPY&#34;]
        )
    pdb_ids = pdb_ids.exec(&#34;assembly&#34;)

    ordered_folders = [
        x.key
        for x in _s3list(
            boto3.resource(&#34;s3&#34;, config=Config(signature_version=UNSIGNED)).Bucket(
                &#34;pdbsnapshots&#34;
            ),
            &#34;&#34;,
            recursive=False,
            list_objs=False,
        )
    ]
    ordered_folders = sorted(
        ordered_folders, reverse=True
    )  # a list of PDB snapshots from newest to oldest
    if pdb_snapshot is not None:
        if pdb_snapshot not in ordered_folders:
            raise ValueError(
                f&#34;The {pdb_snapshot} PDB snapshot not found, please choose from {ordered_folders}&#34;
            )
        ind = ordered_folders.index(pdb_snapshot)
        ordered_folders = ordered_folders[ind:]

    # session = boto3.session.Session()
    # s3_client = session.client(&#34;s3&#34;, config=Config(signature_version=UNSIGNED))

    session = get_session()

    def download_live(id):
        pdb_id, biounit = id.split(&#34;-&#34;)
        filenames = {
            &#34;cif&#34;: f&#34;{pdb_id}-assembly{biounit}.cif.gz&#34;,
            &#34;pdb&#34;: f&#34;{pdb_id}.pdb{biounit}.gz&#34;,
        }
        for t in filenames:
            local_path = os.path.join(tmp_folder, f&#34;{pdb_id}-{biounit}&#34;) + f&#34;.{t}.gz&#34;
            try:
                url = f&#34;https://files.rcsb.org/download/{filenames[t]}&#34;
                response = requests.get(url)
                open(local_path, &#34;wb&#34;).write(response.content)
                return local_path
            except:
                pass
        return id

    def download_fasta_f(pdb_id, datadir):
        downloadurl = &#34;https://www.rcsb.org/fasta/entry/&#34;
        pdbfn = pdb_id + &#34;/download&#34;
        outfnm = os.path.join(datadir, f&#34;{pdb_id.lower()}.fasta&#34;)

        url = downloadurl + pdbfn
        try:
            urllib.request.urlretrieve(url, outfnm)
            return outfnm

        except Exception as err:
            # print(str(err), file=sys.stderr)
            return None

    def process_f(
        local_path,
        show_error=False,
        force=True,
    ):
        try:
            # local_path = download_f(pdb_id, s3_client=s3_client, load_live=load_live)
            fn = os.path.basename(local_path)
            pdb_id = fn.split(&#34;.&#34;)[0]
            target_file = os.path.join(OUTPUT_FOLDER, pdb_id + &#34;.pickle&#34;)
            if not force and os.path.exists(target_file):
                raise PDBError(&#34;File already exists&#34;)
            pdb_dict = _open_structure(
                local_path,
                tmp_folder=TMP_FOLDER,
            )
            # filter and convert
            pdb_dict = _align_structure(
                pdb_dict,
                min_length=MIN_LENGTH,
                max_length=MAX_LENGTH,
                max_missing_ends=MISSING_ENDS_THR,
                max_missing_middle=MISSING_MIDDLE_THR,
            )
            # save
            if pdb_dict is not None:
                with open(target_file, &#34;wb&#34;) as f:
                    pickle.dump(pdb_dict, f)
        except Exception as e:
            if show_error:
                raise e
            else:
                _log_exception(e, LOG_FILE, pdb_id, TMP_FOLDER)

    # for x in [&#34;1a52-3&#34;, &#34;1a52-4&#34;, &#34;1a52-2&#34;, &#34;1a52-1&#34;]:
    #     process_f(x, show_error=True, force=force)

    try:
        with ThreadPoolExecutor(max_workers=8) as executor:
            print(&#34;Get a file list...&#34;)
            ids = []
            for i, x in enumerate(tqdm(pdb_ids)):
                ids.append(x)
                if n is not None and i == n:
                    break
            print(&#34;Download fasta files...&#34;)
            pdbs = set([x.split(&#34;-&#34;)[0] for x in ids])
            future_to_key = {
                executor.submit(
                    lambda x: download_fasta_f(x, datadir=tmp_folder), key
                ): key
                for key in pdbs
            }
            _ = [
                x.result()
                for x in tqdm(futures.as_completed(future_to_key), total=len(pdbs))
            ]

        # _ = [process_f(x, force=force, load_live=load_live) for x in tqdm(ids)]
        print(&#34;Download structure files...&#34;)
        paths = _download_s3_parallel(
            pdb_ids=ids, tmp_folder=tmp_folder, snapshots=[ordered_folders[0]]
        )
        paths = [item for sublist in paths for item in sublist]
        error_ids = [x for x in paths if not x.endswith(&#34;.gz&#34;)]
        paths = [x for x in paths if x.endswith(&#34;.gz&#34;)]
        if load_live:
            print(&#34;Download newest structure files...&#34;)
            live_paths = p_map(download_live, error_ids)
            error_ids = []
            for x in live_paths:
                if x.endswith(&#34;.gz&#34;):
                    paths.append(x)
                else:
                    error_ids.append(x)
        for id in error_ids:
            with open(LOG_FILE, &#34;a&#34;) as f:
                f.write(f&#34;&lt;&lt;&lt; Could not download PDB/mmCIF file: {id} \n&#34;)
        print(&#34;Filter and process...&#34;)
        _ = p_map(lambda x: process_f(x, force=force), paths)
    except Exception as e:
        _raise_rcsbsearch(e)

    stats = get_error_summary(LOG_FILE, verbose=False)
    not_found_error = &#34;&lt;&lt;&lt; PDB / mmCIF file downloaded but not found&#34;
    while not_found_error in stats:
        with open(LOG_FILE, &#34;r&#34;) as f:
            lines = [x for x in f.readlines() if not x.startswith(not_found_error)]
        os.remove(LOG_FILE)
        with open(f&#34;{LOG_FILE}_tmp&#34;, &#34;a&#34;) as f:
            for line in lines:
                f.write(line)
        _ = p_map(lambda x: process_f(x, force=force), stats[not_found_error])
        stats = get_error_summary(LOG_FILE, verbose=False)
    if os.path.exists(f&#34;{LOG_FILE}_tmp&#34;):
        with open(LOG_FILE, &#34;r&#34;) as f:
            lines = [x for x in f.readlines() if not x.startswith(not_found_error)]
        os.remove(LOG_FILE)
        with open(f&#34;{LOG_FILE}_tmp&#34;, &#34;a&#34;) as f:
            for line in lines:
                f.write(line)
        os.rename(f&#34;{LOG_FILE}_tmp&#34;, LOG_FILE)

    if remove_redundancies:
        removed = _remove_database_redundancies(
            OUTPUT_FOLDER, seq_identity_threshold=seq_identity_threshold
        )
        _log_removed(removed, LOG_FILE)

    return get_error_summary(LOG_FILE)


class _PadCollate:
    &#34;&#34;&#34;
    A variant of `collate_fn` that pads according to the longest sequence in
    a batch of sequences

    If `mask_residues` is `True`, an additional `&#39;masked_res&#39;` key is added to the output. The value is a binary
    tensor where 1 denotes the part that needs to be predicted and 0 is everything else. The tensors are generated
    according to the following rules:
    - if `mask_whole_chains` is `True`, the whole chain is masked
    - if `mask_frac` is given, the number of residues to mask is `mask_frac` times the length of the chain,
    - otherwise, the number of residues to mask is sampled uniformly from the range [`lower_limit`, `upper_limit`].

    If `force_binding_sites_frac` &gt; 0 and `mask_whole_chains` is `False`, in the fraction of cases where a chain
    from a polymer is sampled, the center of the masked region will be forced to be in a binding site.
    &#34;&#34;&#34;

    def __init__(
        self,
        mask_residues=True,
        lower_limit=15,
        upper_limit=100,
        mask_frac=None,
        mask_whole_chains=False,
        force_binding_sites_frac=0.15,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        batch : dict
            a batch generated by `ProteinDataset` and `PadCollate`
        lower_limit : int, default 15
            the minimum number of residues to mask
        upper_limit : int, default 100
            the maximum number of residues to mask
        mask_frac : float, optional
            if given, the `lower_limit` and `upper_limit` are ignored and the number of residues to mask is `mask_frac` times the length of the chain
        mask_whole_chains : bool, default False
            if `True`, `upper_limit`, `force_binding_sites` and `lower_limit` are ignored and the whole chain is masked instead
        force_binding_sites_frac : float, default 0.15
            if &gt; 0, in the fraction of cases where a chain from a polymer is sampled, the center of the masked region will be
            forced to be in a binding site

        Returns
        -------
        chain_M : torch.Tensor
            a `(B, L)` shaped binary tensor where 1 denotes the part that needs to be predicted and
            0 is everything else
        &#34;&#34;&#34;

        super().__init__()
        self.mask_residues = mask_residues
        self.lower_limit = lower_limit
        self.upper_limit = upper_limit
        self.mask_frac = mask_frac
        self.mask_whole_chains = mask_whole_chains
        self.force_binding_sites_frac = force_binding_sites_frac

    def _get_masked_sequence(
        self,
        batch,
    ):
        &#34;&#34;&#34;
        Get the mask for the residues that need to be predicted

        Depending on the parameters the residues are selected as follows:
        - if `mask_whole_chains` is `True`, the whole chain is masked
        - if `mask_frac` is given, the number of residues to mask is `mask_frac` times the length of the chain,
        - otherwise, the number of residues to mask is sampled uniformly from the range [`lower_limit`, `upper_limit`].

        If `force_binding_sites_frac` &gt; 0 and `mask_whole_chains` is `False`, in the fraction of cases where a chain
        from a polymer is sampled, the center of the masked region will be forced to be in a binding site.

        Parameters
        ----------
        batch : dict
            a batch generated by `ProteinDataset` and `PadCollate`
        lower_limit : int, default 15
            the minimum number of residues to mask
        upper_limit : int, default 100
            the maximum number of residues to mask
        mask_frac : float, optional
            if given, the `lower_limit` and `upper_limit` are ignored and the number of residues to mask is `mask_frac` times the length of the chain
        mask_whole_chains : bool, default False
            if `True`, `upper_limit`, `force_binding_sites` and `lower_limit` are ignored and the whole chain is masked instead
        force_binding_sites_frac : float, default 0.15
            if &gt; 0, in the fraction of cases where a chain from a polymer is sampled, the center of the masked region will be
            forced to be in a binding site

        Returns
        -------
        chain_M : torch.Tensor
            a `(B, L)` shaped binary tensor where 1 denotes the part that needs to be predicted and
            0 is everything else
        &#34;&#34;&#34;

        chain_M = torch.zeros(batch[&#34;S&#34;].shape)
        for i, coords in enumerate(batch[&#34;X&#34;]):
            chain_index = batch[&#34;chain_id&#34;][i]
            chain_bool = batch[&#34;chain_encoding_all&#34;][i] == chain_index
            if self.mask_whole_chains:
                chain_M[i, chain_bool] = 1
            else:
                chains = torch.unique(batch[&#34;chain_encoding_all&#34;][i])
                chain_start = torch.where(chain_bool)[0][0]
                chain = coords[chain_bool]
                res_i = None
                if len(chains) &gt; 1 and self.force_binding_sites_frac &gt; 0:
                    if random.uniform(0, 1) &lt; self.force_binding_sites_frac:
                        intersection_indices = []
                        for chain_id in chains:
                            if chain_id == chain_index:
                                continue
                            chain_id_bool = batch[&#34;chain_encoding_all&#34;][i] == chain_id
                            chain_min = (
                                coords[chain_id_bool][:, 2, :].min(0)[0].unsqueeze(0)
                            )
                            chain_max = (
                                coords[chain_id_bool][:, 2, :].max(0)[0].unsqueeze(0)
                            )
                            min_mask = (chain[:, 2, :] &gt;= chain_min - 4).sum(-1) == 3
                            max_mask = (chain[:, 2, :] - 4 &lt;= chain_max).sum(-1) == 3
                            intersection_indices += torch.where(min_mask * max_mask)[
                                0
                            ].tolist()
                        if len(intersection_indices) &gt; 0:
                            res_i = intersection_indices[
                                random.randint(0, len(intersection_indices) - 1)
                            ]
                if res_i is None:
                    non_zero = torch.where(batch[&#34;mask&#34;][i][chain_bool])[0]
                    res_i = non_zero[random.randint(0, len(non_zero) - 1)]
                res_coords = chain[res_i, 2, :]
                neighbor_indices = torch.where(batch[&#34;mask&#34;][i][chain_bool])[0]
                if self.mask_frac is not None:
                    assert self.mask_frac &gt; 0 and self.mask_frac &lt; 1
                    k = int(len(neighbor_indices) * self.mask_frac)
                    k = max(k, 10)
                else:
                    up = min(
                        self.upper_limit, int(len(neighbor_indices) * 0.5)
                    )  # do not mask more than half of the sequence
                    low = min(up - 1, self.lower_limit)
                    k = random.choice(range(low, up))
                dist = torch.sum(
                    (chain[neighbor_indices, 1, :] - res_coords.unsqueeze(0)) ** 2, -1
                )
                closest_indices = neighbor_indices[
                    torch.topk(dist, k, largest=False)[1]
                ]
                chain_M[i, closest_indices + chain_start] = 1
        return chain_M

    def pad_collate(self, batch):
        # find longest sequence
        out = {}
        max_len = max(map(lambda x: x[&#34;S&#34;].shape[0], batch))

        # pad according to max_len
        to_pad = [max_len - b[&#34;S&#34;].shape[0] for b in batch]
        for key in batch[0].keys():
            if key in [&#34;chain_id&#34;, &#34;chain_dict&#34;]:
                continue
            out[key] = torch.stack(
                [
                    torch.cat([b[key], torch.zeros((pad, *b[key].shape[1:]))], 0)
                    for b, pad in zip(batch, to_pad)
                ],
                0,
            )
        out[&#34;chain_id&#34;] = torch.tensor([b[&#34;chain_id&#34;] for b in batch])
        out[&#34;masked_res&#34;] = self._get_masked_sequence(out)
        return out

    def __call__(self, batch):
        return self.pad_collate(batch)


def download_data(tag, local_datasets_folder=&#34;./data&#34;, skip_splitting=False):
    &#34;&#34;&#34;
    Download a pre-computed dataset with train/test/validation splits

    Parameters
    ----------
    tag : str
        the name of the dataset to load
    local_datasets_folder : str, default &#34;./data&#34;
        the path to the folder that will store proteinflow datasets, logs and temporary files
    skip_splitting : bool, default False
        if `True`, skip the split dictionary creation and the file moving steps
    &#34;&#34;&#34;

    data_path = _download_dataset(tag, local_datasets_folder)
    if not skip_splitting:
        _split_data(data_path)


def generate_data(
    tag,
    local_datasets_folder=&#34;./data&#34;,
    min_length=30,
    max_length=10000,
    resolution_thr=3.5,
    missing_ends_thr=0.3,
    missing_middle_thr=0.1,
    not_filter_methods=False,
    not_remove_redundancies=False,
    skip_splitting=False,
    seq_identity_threshold=0.9,
    n=None,
    force=False,
    split_tolerance=0.2,
    test_split=0.05,
    valid_split=0.05,
    pdb_snapshot=None,
    load_live=False,
    min_seq_id=0.3,
):
    &#34;&#34;&#34;
    Download and parse PDB files that meet filtering criteria

    The output files are pickled nested dictionaries where first-level keys are chain Ids and second-level keys are
    the following:

    - `&#39;crd_bb&#39;`: a `numpy` array of shape `(L, 4, 3)` with backbone atom coordinates (N, C, CA, O),
    - `&#39;crd_sc&#39;`: a `numpy` array of shape `(L, 10, 3)` with sidechain atom coordinates (in a fixed order, check `sidechain_order()`),
    - `&#39;msk&#39;`: a `numpy` array of shape `(L,)` where ones correspond to residues with known coordinates and
        zeros to missing values,
    - `&#39;seq&#39;`: a string of length `L` with residue types.

    All errors including reasons for filtering a file out are logged in the log file.

    Parameters
    ----------
    tag : str
        the name of the dataset to load
    local_datasets_folder : str, default &#34;./data&#34;
        the path to the folder that will store proteinflow datasets, logs and temporary files
    min_length : int, default 30
        The minimum number of non-missing residues per chain
    max_length : int, default 10000
        The maximum number of residues per chain (set None for no threshold)
    resolution_thr : float, default 3.5
        The maximum resolution
    missing_ends_thr : float, default 0.3
        The maximum fraction of missing residues at the ends
    missing_middle_thr : float, default 0.1
        The maximum fraction of missing values in the middle (after missing ends are disregarded)
    not_filter_methods : bool, default False
        If `False`, only files obtained with X-ray or EM will be processed
    not_remove_redundancies : bool, default False
        If &#39;False&#39;, removes biounits that are doubles of others sequence wise
    skip_splitting : bool, default False
        if `True`, skip the split dictionary creation and the file moving steps
    seq_identity_threshold : float, default 0.9
        The threshold upon which sequences are considered as one and the same (default: 90%)
    n : int, default None
        The number of files to process (for debugging purposes)
    force : bool, default False
        When `True`, rewrite the files if they already exist
    split_tolerance : float, default 0.2
        The tolerance on the split ratio (default 20%)
    test_split : float, default 0.05
        The percentage of chains to put in the test set (default 5%)
    valid_split : float, default 0.05
        The percentage of chains to put in the validation set (default 5%)
    pdb_snapshot : str, optional
        the PDB snapshot to use, by default the latest is used
    load_live : bool, default False
        if `True`, load the files that are not in the latest PDB snapshot from the PDB FTP server (forced to `False` if `pdb_snapshot` is not `None`)
    min_seq_id : float in [0, 1], default 0.3
        minimum sequence identity for `mmseqs`

    Returns
    -------
    log : dict
        a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors

    &#34;&#34;&#34;
    _check_mmseqs()
    filter_methods = not not_filter_methods
    remove_redundancies = not not_remove_redundancies
    tmp_folder = os.path.join(local_datasets_folder, &#34;tmp&#34;)
    output_folder = os.path.join(local_datasets_folder, f&#34;proteinflow_{tag}&#34;)
    log_folder = os.path.join(local_datasets_folder, &#34;logs&#34;)
    out_split_dict_folder = os.path.join(output_folder, &#34;splits_dict&#34;)

    log_dict = _run_processing(
        tmp_folder=tmp_folder,
        output_folder=output_folder,
        log_folder=log_folder,
        min_length=min_length,
        max_length=max_length,
        resolution_thr=resolution_thr,
        missing_ends_thr=missing_ends_thr,
        missing_middle_thr=missing_middle_thr,
        filter_methods=filter_methods,
        remove_redundancies=remove_redundancies,
        seq_identity_threshold=seq_identity_threshold,
        n=n,
        force=force,
        tag=tag,
        pdb_snapshot=pdb_snapshot,
        load_live=load_live,
    )
    if not skip_splitting:
        _get_split_dictionaries(
            tmp_folder=tmp_folder,
            output_folder=output_folder,
            split_tolerance=split_tolerance,
            test_split=test_split,
            valid_split=valid_split,
            out_split_dict_folder=out_split_dict_folder,
            min_seq_id=min_seq_id,
        )

        _split_data(output_folder)
    return log_dict


def split_data(
    tag,
    local_datasets_folder=&#34;./data&#34;,
    split_tolerance=0.2,
    test_split=0.05,
    valid_split=0.05,
    ignore_existing=False,
    min_seq_id=0.3,
):
    &#34;&#34;&#34;
    Split `proteinflow` entry files into training, test and validation.

    Our splitting algorithm has two objectives: achieving minimal data leakage and balancing the proportion of
    single chain, homomer and heteromer entries.

    It follows these steps:

    1. cluster chains by sequence identity,
    2. generate a graph where nodes are the clusters and edges are protein-protein interactions between chains
    from those clusters,
    3. split connected components of the graph into training, test and validation subsets while keeping the proportion
    of single chains, homomers and heteromers close to that in the full dataset (within `split_tolerance`).


    Parameters
    ----------
    tag : str
        the name of the dataset to load
    local_datasets_folder : str, default &#34;./data&#34;
        the path to the folder that will store proteinflow datasets, logs and temporary files
    split_tolerance : float, default 0.2
        The tolerance on the split ratio (default 20%)
    test_split : float, default 0.05
        The percentage of chains to put in the test set (default 5%)
    valid_split : float, default 0.05
        The percentage of chains to put in the validation set (default 5%)
    ignore_existing : bool, default False
        If `True`, overwrite existing dictionaries for this tag; otherwise, load the existing dictionary
    min_seq_id : float in [0, 1], default 0.3
        minimum sequence identity for `mmseqs`

    Returns
    -------
    log : dict
        a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors
    &#34;&#34;&#34;

    _check_mmseqs()
    tmp_folder = os.path.join(local_datasets_folder, &#34;tmp&#34;)
    output_folder = os.path.join(local_datasets_folder, f&#34;proteinflow_{tag}&#34;)
    out_split_dict_folder = os.path.join(output_folder, &#34;splits_dict&#34;)
    exists = False

    if os.path.exists(out_split_dict_folder):
        if not ignore_existing:
            warnings.warn(
                f&#34;Found an existing dictionary for tag {tag}. proteinflow will load it and ignore the parameters! Run with --ignore_existing to overwrite.&#34;
            )
            exists = True
    if not exists:
        _get_split_dictionaries(
            tmp_folder=tmp_folder,
            output_folder=output_folder,
            split_tolerance=split_tolerance,
            test_split=test_split,
            valid_split=valid_split,
            out_split_dict_folder=out_split_dict_folder,
            min_seq_id=min_seq_id,
        )

    _split_data(output_folder)


class ProteinDataset(Dataset):
    &#34;&#34;&#34;
    Dataset to load proteinflow data

    Saves the model input tensors as pickle files in `features_folder`. When `clustering_dict_path` is provided,
    at each iteration a random bionit from a cluster is sampled.

    If a complex contains multiple chains, they are concatenated. The sequence identity information is preserved in the
    `&#39;chain_encoding_all&#39;` object and in the `&#39;residue_idx&#39;` arrays the chain change is denoted by a +100 jump.

    Returns dictionaries with the following keys and values (all values are `torch` tensors):

    - `&#39;X&#39;`: 3D coordinates of N, C, Ca, O, `(total_L, 4, 3)`,
    - `&#39;S&#39;`: sequence indices (shape `(total_L)`),
    - `&#39;mask&#39;`: residue mask (0 where coordinates are missing, 1 otherwise; with interpolation 0s are replaced with 1s), `(total_L)`,
    - `&#39;mask_original&#39;`: residue mask (0 where coordinates are missing, 1 otherwise; not changed with interpolation), `(total_L)`,
    - `&#39;residue_idx&#39;`: residue indices (from 0 to length of sequence, +100 where chains change), `(total_L)`,
    - `&#39;chain_encoding_all&#39;`: chain indices, `(total_L)`,
    - `&#39;chain_id`&#39;: a sampled chain index,
    - `&#39;chain_dict&#39;`: a dictionary of chain ids (keys are chain ids, e.g. `&#39;A&#39;`, values are the indices used in `&#39;chain_id&#39;` and `&#39;chain_encoding_all&#39;` objects)

    You can also choose to include additional features (set in the `node_features_type` parameter):

    - `&#39;sidechain_orientation&#39;`: a unit vector in the direction of the sidechain, `(total_L, 3)`,
    - `&#39;dihedral&#39;`: the dihedral angles, `(total_L, 2)`,
    - `&#39;chemical&#39;`: hydropathy, volume, charge, polarity, acceptor/donor features, `(total_L, 6)`,
    - `&#39;secondary_structure&#39;`: a one-hot encoding of secondary structure ([alpha-helix, beta-sheet, coil]), `(total_L, 3)`,
    - `&#39;sidechain_coords&#39;`: the coordinates of the sidechain atoms (see `proteinflow.sidechain_order()` for the order), `(total_L, 10, 3)`,

    In order to compute additional features, use the `feature_functions` parameter. It should be a dictionary with keys
    corresponding to the feature names and values corresponding to the functions that compute the features. The functions
    should take a chain dictionary and an integer representation of the sequence as input (the dictionary is in `proteinflow` format,
    see the docs for `generate_data` for details) and return a `numpy` array shaped as `(#residues, #features)`.

    &#34;&#34;&#34;

    def __init__(
        self,
        dataset_folder,
        features_folder=&#34;./data/tmp/&#34;,
        clustering_dict_path=None,
        max_length=None,
        rewrite=False,
        use_fraction=1,
        load_to_ram=False,
        debug=False,
        interpolate=&#34;none&#34;,
        node_features_type=&#34;zeros&#34;,
        debug_file_path=None,
        entry_type=&#34;biounit&#34;,  # biounit, chain, pair
        classes_to_exclude=None,  # heteromers, homomers, single_chains
        shuffle_clusters=True,
        feature_functions=None,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        dataset_folder : str
            the path to the folder with proteinflow format input files (assumes that files are named {biounit_id}.pickle)
        features_folder : str, default &#34;./data/tmp/&#34;
            the path to the folder where the ProteinMPNN features will be saved
        clustering_dict_path : str, optional
            path to the pickled clustering dictionary (keys are cluster ids, values are (biounit id, chain id) tuples)
        max_length : int, optional
            entries with total length of chains larger than `max_length` will be disregarded
        rewrite : bool, default False
            if `False`, existing feature files are not overwritten
        use_fraction : float, default 1
            the fraction of the clusters to use (first N in alphabetic order)
        load_to_ram : bool, default False
            if `True`, the data will be stored in RAM (use with caution! if RAM isn&#39;t big enough the machine might crash)
        debug : bool, default False
            only process 1000 files
        interpolate : {&#34;none&#34;, &#34;only_middle&#34;, &#34;all&#34;}
            `&#34;none&#34;` for no interpolation, `&#34;only_middle&#34;` for only linear interpolation in the middle, `&#34;all&#34;` for linear interpolation + ends generation
        node_features_type : {&#34;zeros&#34;, &#34;dihedral&#34;, &#34;sidechain_orientation&#34;, &#34;chemical&#34;, &#34;secondary_structure&#34; or combinations with &#34;+&#34;}
            the type of node features, e.g. `&#34;dihedral&#34;` or `&#34;sidechain_orientation+chemical&#34;`
        debug_file_path : str, optional
            if not `None`, open this single file instead of loading the dataset
        entry_type : {&#34;biounit&#34;, &#34;chain&#34;, &#34;pair&#34;}
            the type of entries to generate (`&#34;biounit&#34;` for biounit-level complexes, `&#34;chain&#34;` for chain-level, `&#34;pair&#34;`
            for chain-chain pairs (all pairs that are seen in the same biounit and have intersecting coordinate clouds))
        classes_to_exclude : list of str, optional
            a list of classes to exclude from the dataset (select from `&#34;single_chains&#34;`, `&#34;heteromers&#34;`, `&#34;homomers&#34;`)
        shuffle_clusters : bool, default True
            if `True`, a new representative is randomly selected for each cluster at each epoch (if `clustering_dict_path` is given)
        feature_functions : dict, optional
            a dictionary of functions to compute additional features (keys are the names of the features, values are the functions)
        &#34;&#34;&#34;

        alphabet = ALPHABET
        self.alphabet_dict = defaultdict(lambda: 0)
        for i, letter in enumerate(alphabet):
            self.alphabet_dict[letter] = i
        self.alphabet_dict[&#34;X&#34;] = 0
        self.files = defaultdict(lambda: defaultdict(list))  # file path by biounit id
        self.loaded = None
        self.dataset_folder = dataset_folder
        self.features_folder = features_folder
        self.feature_types = []
        if node_features_type is not None:
            self.feature_types = node_features_type.split(&#34;+&#34;)
        self.entry_type = entry_type
        self.shuffle_clusters = shuffle_clusters
        self.feature_functions = {
            &#34;sidechain_orientation&#34;: self._sidechain,
            &#34;dihedral&#34;: self._dihedral,
            &#34;chemical&#34;: self._chemical,
            &#34;secondary_structure&#34;: self._sse,
            &#34;sidechain_coords&#34;: self._sidechain_coords,
        }
        self.feature_functions.update(feature_functions or {})
        if classes_to_exclude is not None and not all(
            [
                x in [&#34;single_chains&#34;, &#34;heteromers&#34;, &#34;homomers&#34;]
                for x in classes_to_exclude
            ]
        ):
            raise ValueError(
                &#34;Invalid class to exclude, choose from &#39;single_chains&#39;, &#39;heteromers&#39;, &#39;homomers&#39;&#34;
            )

        if debug_file_path is not None:
            self.dataset_folder = os.path.dirname(debug_file_path)
            debug_file_path = os.path.basename(debug_file_path)

        self.main_atom_dict = defaultdict(lambda: None)
        d1to3 = {v: k for k, v in D3TO1.items()}
        for i, letter in enumerate(alphabet):
            if i == 0:
                continue
            self.main_atom_dict[i] = MAIN_ATOMS[d1to3[letter]]

        # create feature folder if it does not exist
        if not os.path.exists(self.features_folder):
            os.makedirs(self.features_folder)

        self.interpolate = interpolate
        # generate the feature files
        print(&#34;Processing files...&#34;)
        if debug_file_path is None:
            to_process = os.listdir(dataset_folder)
        else:
            to_process = [debug_file_path]
        if clustering_dict_path is not None and use_fraction &lt; 1:
            with open(clustering_dict_path, &#34;rb&#34;) as f:
                clusters = pickle.load(f)
            keys = sorted(clusters.keys())[: int(len(clusters) * use_fraction)]
            to_process = set()
            for key in keys:
                to_process.update([x[0] for x in clusters[key]])
            to_process = list(to_process)
        if debug:
            to_process = to_process[:1000]
        # output_tuples = [self._process(x, rewrite=rewrite) for x in tqdm(to_process)]
        if self.entry_type == &#34;pair&#34;:
            print(
                &#34;Please note that the pair entry type takes longer to process than the other two. The progress bar is not linear because of the varying number of chains per file.&#34;
            )
        output_tuples_list = p_map(
            lambda x: self._process(x, rewrite=rewrite, max_length=max_length),
            to_process,
        )
        # save the file names
        for output_tuples in output_tuples_list:
            for id, filename, chain_set in output_tuples:
                for chain in chain_set:
                    self.files[id][chain].append(filename)
        # filter by length
        # seen = set()
        # if max_length is not None:
        #     to_remove = []
        #     for id, chain_dict in self.files.items():
        #         for chain, file_list in chain_dict.items():
        #             for file in file_list:
        #                 if file in seen:
        #                     continue
        #                 seen.add(file)
        #                 with open(file, &#34;rb&#34;) as f:
        #                     data = pickle.load(f)
        #                     if len(data[&#34;S&#34;]) &gt; max_length:
        #                         to_remove.append(file)
        #     for id in list(self.files.keys()):
        #         chain_dict = self.files[id]
        #         for chain in list(chain_dict.keys()):
        #             file_list = chain_dict[chain]
        #             for file in file_list:
        #                 if file in to_remove:
        #                     self.files[id][chain].remove(file)
        #                     if len(self.files[id][chain]) == 0:
        #                         self.files[id].pop(chain)
        #                     if len(self.files[id]) == 0:
        #                         self.files.pop(id)
        # load the clusters
        if classes_to_exclude is None:
            classes_to_exclude = []
        elif clustering_dict_path is None:
            raise ValueError(
                &#34;classes_to_exclude is not None, but clustering_dict_path is None&#34;
            )
        if clustering_dict_path is not None:
            if entry_type == &#34;pair&#34;:
                classes_to_exclude = set(classes_to_exclude)
                classes_to_exclude.add(&#34;single_chains&#34;)
                classes_to_exclude = list(classes_to_exclude)
            with open(clustering_dict_path, &#34;rb&#34;) as f:
                self.clusters = pickle.load(f)  # list of biounit ids by cluster id
                classes = pickle.load(f)
            to_exclude = set()
            for c in classes_to_exclude:
                for key, id_arr in classes.get(c, {}).items():
                    for id, _ in id_arr:
                        to_exclude.add(id)
            for key in list(self.clusters.keys()):
                cluster_list = []
                for x in self.clusters[key]:
                    if x[0] in to_exclude:
                        continue
                    id = x[0].split(&#34;.&#34;)[0]
                    chain = x[1]
                    if id not in self.files:
                        continue
                    if chain not in self.files[id]:
                        continue
                    if len(self.files[id][chain]) == 0:
                        continue
                    cluster_list.append([id, chain])
                self.clusters[key] = cluster_list
                if len(self.clusters[key]) == 0:
                    self.clusters.pop(key)
            self.data = list(self.clusters.keys())
        else:
            self.clusters = None
            self.data = list(self.files.keys())
        # create a smaller dataset if necessary
        if clustering_dict_path is None and use_fraction &lt; 1:
            self.data = sorted(self.data)[: int(len(self.data) * use_fraction)]
        if load_to_ram:
            print(&#34;Loading to RAM...&#34;)
            self.loaded = {}
            seen = set()
            for id in self.files:
                for chain, file_list in self.files[id].items():
                    for file in file_list:
                        if file in seen:
                            continue
                        seen.add(file)
                        with open(file, &#34;rb&#34;) as f:
                            self.loaded[file] = pickle.load(f)

    def _interpolate(self, crd_i, mask_i):
        &#34;&#34;&#34;
        Fill in missing values in the middle with linear interpolation and (if fill_ends is true) build an initialization for the ends

        For the ends, the first 10 residues are 3.6 A apart from each other on a straight line from the last known value away from the center.
        Next they are 3.6 A apart in a random direction.
        &#34;&#34;&#34;

        if self.interpolate in [&#34;all&#34;, &#34;only_middle&#34;]:
            crd_i[(1 - mask_i).astype(bool)] = np.nan
            df = pd.DataFrame(crd_i.reshape((crd_i.shape[0], -1)))
            crd_i = df.interpolate(limit_area=&#34;inside&#34;).values.reshape(crd_i.shape)
        if self.interpolate == &#34;all&#34;:
            non_nans = np.where(~np.isnan(crd_i[:, 0, 0]))[0]
            known_start = non_nans[0]
            known_end = non_nans[-1] + 1
            if known_end &lt; len(crd_i) or known_start &gt; 0:
                center = crd_i[non_nans, 2, :].mean(0)
                if known_start &gt; 0:
                    direction = crd_i[known_start, 2, :] - center
                    direction = direction / linalg.norm(direction)
                    for i in range(0, min(known_start, 10)):
                        crd_i[known_start - i - 1] = (
                            crd_i[known_start - i] + direction * 3.6
                        )
                    for i in range(min(known_start, 10), known_start):
                        v = np.random.rand(3)
                        v = v / linalg.norm(v)
                        crd_i[known_start - i - 1] = crd_i[known_start - i] + v * 3.6
                if known_end &lt; len(crd_i):
                    to_add = len(crd_i) - known_end
                    direction = crd_i[known_end - 1, 2, :] - center
                    direction = direction / linalg.norm(direction)
                    for i in range(0, min(to_add, 10)):
                        crd_i[known_end + i] = (
                            crd_i[known_end + i - 1] + direction * 3.6
                        )
                    for i in range(min(to_add, 10), to_add):
                        v = np.random.rand(3)
                        v = v / linalg.norm(v)
                        crd_i[known_end + i] = crd_i[known_end + i - 1] + v * 3.6
            mask_i = np.ones(mask_i.shape)
        if self.interpolate in [&#34;only_middle&#34;]:
            nan_mask = np.isnan(crd_i)  # in the middle the nans have been interpolated
            mask_i[~np.isnan(crd_i[:, 0, 0])] = 1
            crd_i[nan_mask] = 0
        if self.interpolate == &#34;zeros&#34;:
            non_nans = np.where(mask_i != 0)[0]
            known_start = non_nans[0]
            known_end = non_nans[-1] + 1
            mask_i[known_start:known_end] = 1
        return crd_i, mask_i

    def _dihedral_angle(self, crd, msk):
        &#34;&#34;&#34;Praxeolitic formula
        1 sqrt, 1 cross product&#34;&#34;&#34;

        p0 = crd[..., 0, :]
        p1 = crd[..., 1, :]
        p2 = crd[..., 2, :]
        p3 = crd[..., 3, :]

        b0 = -1.0 * (p1 - p0)
        b1 = p2 - p1
        b2 = p3 - p2

        b1 /= np.expand_dims(np.linalg.norm(b1, axis=-1), -1) + 1e-7

        v = b0 - np.expand_dims(np.einsum(&#34;bi,bi-&gt;b&#34;, b0, b1), -1) * b1
        w = b2 - np.expand_dims(np.einsum(&#34;bi,bi-&gt;b&#34;, b2, b1), -1) * b1

        x = np.einsum(&#34;bi,bi-&gt;b&#34;, v, w)
        y = np.einsum(&#34;bi,bi-&gt;b&#34;, np.cross(b1, v), w)
        dh = np.degrees(np.arctan2(y, x))
        dh[1 - msk] = 0
        return dh

    def _dihedral(self, chain_dict, seq):
        &#34;&#34;&#34;
        Dihedral angles
        &#34;&#34;&#34;

        crd = chain_dict[&#34;crd_bb&#34;]
        msk = chain_dict[&#34;msk&#34;]
        angles = []
        # N, C, Ca, O
        # psi
        p = crd[:-1, [0, 2, 1], :]
        p = np.concatenate([p, crd[1:, [0], :]], 1)
        p = np.pad(p, ((0, 1), (0, 0), (0, 0)))
        angles.append(self._dihedral_angle(p, msk))
        # phi
        p = crd[:-1, [1], :]
        p = np.concatenate([p, crd[1:, [0, 2, 1]]], 1)
        p = np.pad(p, ((1, 0), (0, 0), (0, 0)))
        angles.append(self._dihedral_angle(p, msk))
        angles = np.stack(angles, -1)
        return angles

    def _sidechain(self, chain_dict, seq):
        &#34;&#34;&#34;
        Sidechain orientation (defined by the &#39;main atoms&#39; in the `main_atom_dict` dictionary)
        &#34;&#34;&#34;

        crd_sc = chain_dict[&#34;crd_sc&#34;]
        crd_bb = chain_dict[&#34;crd_bb&#34;]
        orientation = np.zeros((crd_sc.shape[0], 3))
        for i in range(1, 21):
            if self.main_atom_dict[i] is not None:
                orientation[seq == i] = (
                    crd_sc[seq == i, self.main_atom_dict[i], :] - crd_bb[seq == i, 2, :]
                )
            else:
                S_mask = seq == i
                orientation[S_mask] = np.random.rand(*orientation[S_mask].shape)
        orientation /= np.expand_dims(linalg.norm(orientation, axis=-1), -1) + 1e-7
        return orientation

    def _chemical(self, chain_dict, seq):
        &#34;&#34;&#34;
        Chemical features (hydropathy, volume, charge, polarity, acceptor/donor)
        &#34;&#34;&#34;

        features = np.array([_PMAP(x) for x in seq])
        return features

    def _sse(self, chain_dict, seq):
        &#34;&#34;&#34;
        Secondary structure features
        &#34;&#34;&#34;

        sse_map = {&#34;c&#34;: [0, 0, 1], &#34;b&#34;: [0, 1, 0], &#34;a&#34;: [1, 0, 0], &#34;&#34;: [0, 0, 0]}
        sse = _annotate_sse(chain_dict[&#34;crd_bb&#34;])
        sse = np.array([sse_map[x] for x in sse]) * chain_dict[&#34;msk&#34;][:, None]
        return sse

    def _sidechain_coords(self, chain_dict, seq):
        &#34;&#34;&#34;
        Sidechain coordinates
        &#34;&#34;&#34;

        crd_sc = chain_dict[&#34;crd_sc&#34;]
        return crd_sc

    def _process(self, filename, rewrite=False, max_length=None):
        &#34;&#34;&#34;
        Process a proteinflow file and save it as ProteinMPNN features
        &#34;&#34;&#34;

        input_file = os.path.join(self.dataset_folder, filename)
        no_extension_name = filename.split(&#34;.&#34;)[0]
        try:
            with open(input_file, &#34;rb&#34;) as f:
                data = pickle.load(f)
        except:
            print(f&#34;{input_file=}&#34;)
        chains = sorted(data.keys())
        if self.entry_type == &#34;biounit&#34;:
            chain_sets = [chains]
        elif self.entry_type == &#34;chain&#34;:
            chain_sets = [[x] for x in chains]
        elif self.entry_type == &#34;pair&#34;:
            chain_sets = list(combinations(chains, 2))
        else:
            raise RuntimeError(
                &#34;Unknown entry type, please choose from [&#39;biounit&#39;, &#39;chain&#39;, &#39;pair&#39;]&#34;
            )
        output_names = []
        for chains_i, chain_set in enumerate(chain_sets):
            output_file = os.path.join(
                self.features_folder, no_extension_name + f&#34;_{chains_i}.pickle&#34;
            )
            pass_set = False
            add_name = True
            if os.path.exists(output_file) and not rewrite:
                pass_set = True
                if max_length is not None:
                    if sum([len(data[x][&#34;seq&#34;]) for x in chain_set]) &gt; max_length:
                        add_name = False
            else:
                X = []
                S = []
                mask = []
                mask_original = []
                chain_encoding_all = []
                residue_idx = []
                node_features = defaultdict(lambda: [])
                last_idx = 0
                chain_dict = {}

                if max_length is not None:
                    if sum([len(data[x][&#34;seq&#34;]) for x in chain_set]) &gt; max_length:
                        pass_set = True
                        add_name = False

                if self.entry_type == &#34;pair&#34;:
                    intersect = []
                    X1 = data[chain_set[0]][&#34;crd_bb&#34;][
                        data[chain_set[0]][&#34;msk&#34;].astype(bool)
                    ]
                    X2 = data[chain_set[1]][&#34;crd_bb&#34;][
                        data[chain_set[1]][&#34;msk&#34;].astype(bool)
                    ]
                    for dim in range(3):
                        min_dim_1 = X1[:, :, dim].min()
                        max_dim_1 = X1[:, :, dim].max()
                        min_dim_2 = X2[:, :, dim].min()
                        max_dim_2 = X2[:, :, dim].max()
                        if min_dim_1 - 4 &lt;= max_dim_2 and max_dim_1 &gt;= min_dim_2 - 4:
                            intersect.append(True)
                        else:
                            intersect.append(False)
                            break
                    if not all(intersect):
                        pass_set = True
                        add_name = False
            if add_name:
                output_names.append(
                    (os.path.basename(no_extension_name), output_file, chain_set)
                )
            if pass_set:
                continue

            for chain_i, chain in enumerate(chain_set):
                seq = torch.tensor([self.alphabet_dict[x] for x in data[chain][&#34;seq&#34;]])
                S.append(seq)
                mask_original.append(deepcopy(data[chain][&#34;msk&#34;]))
                if self.interpolate != &#34;none&#34;:
                    data[chain][&#34;crd_bb&#34;], data[chain][&#34;msk&#34;] = self._interpolate(
                        data[chain][&#34;crd_bb&#34;], data[chain][&#34;msk&#34;]
                    )
                X.append(data[chain][&#34;crd_bb&#34;])
                mask.append(data[chain][&#34;msk&#34;])
                residue_idx.append(torch.arange(len(data[chain][&#34;seq&#34;])) + last_idx)
                last_idx = residue_idx[-1][-1] + 100
                chain_encoding_all.append(torch.ones(len(data[chain][&#34;seq&#34;])) * chain_i)
                chain_dict[chain] = chain_i
                for name in self.feature_types:
                    if name not in self.feature_functions:
                        continue
                    func = self.feature_functions[name]
                    node_features[name].append(func(data[chain], seq))

            out = {}
            out[&#34;X&#34;] = torch.from_numpy(np.concatenate(X, 0))
            out[&#34;S&#34;] = torch.cat(S)
            out[&#34;mask&#34;] = torch.from_numpy(np.concatenate(mask))
            out[&#34;mask_original&#34;] = torch.from_numpy(np.concatenate(mask_original))
            out[&#34;chain_encoding_all&#34;] = torch.cat(chain_encoding_all)
            out[&#34;residue_idx&#34;] = torch.cat(residue_idx)
            out[&#34;chain_dict&#34;] = chain_dict
            for key, value_list in node_features.items():
                out[key] = torch.from_numpy(np.concatenate(value_list))
            with open(output_file, &#34;wb&#34;) as f:
                pickle.dump(out, f)
        return output_names

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        chain_id = None
        if self.clusters is None:
            id = self.data[idx]  # data is already filtered by length
            chain_id = random.choice(list(self.files[id].keys()))
        else:
            cluster = self.data[idx]
            id = None
            chain_n = -1
            # print(f&#39;{self.clusters[cluster]=}&#39;)
            while (
                id is None or len(self.files[id][chain_id]) == 0
            ):  # some IDs can be filtered out by length
                if self.shuffle_clusters:
                    chain_n = random.randint(0, len(self.clusters[cluster]) - 1)
                else:
                    chain_n += 1
                id, chain_id = self.clusters[cluster][
                    chain_n
                ]  # get id and chain from cluster
                # print(f&#39;{id=}, {len(self.files[id][chain_id])=}&#39;)
        file = random.choice(self.files[id][chain_id])
        if self.loaded is None:
            with open(file, &#34;rb&#34;) as f:
                data = pickle.load(f)
        else:
            data = deepcopy(self.loaded[file])
        data[&#34;chain_id&#34;] = data[&#34;chain_dict&#34;][chain_id]
        data.pop(&#34;chain_dict&#34;)
        return data


class ProteinLoader(DataLoader):
    &#34;&#34;&#34;
    A subclass of `torch.data.utils.DataLoader` tuned for the `proteinflow` dataset

    Creates and iterates over an instance of `ProteinDataset`, omitting the `&#39;chain_dict&#39;` keys.
    See the `ProteinDataset` docs for more information.

    If batch size is larger than one, all objects are padded with zeros at the ends to reach the length of the
    longest protein in the batch.

    If `mask_residues` is `True`, an additional `&#39;masked_res&#39;` key is added to the output. The value is a binary
    tensor shaped `(B, L)` where 1 denotes the part that needs to be predicted and 0 is everything else. The tensors are generated
    according to the following rules:
    - if `mask_whole_chains` is `True`, the whole chain is masked
    - if `mask_frac` is given, the number of residues to mask is `mask_frac` times the length of the chain,
    - otherwise, the number of residues to mask is sampled uniformly from the range [`lower_limit`, `upper_limit`].

    If `force_binding_sites_frac` &gt; 0 and `mask_whole_chains` is `False`, in the fraction of cases where a chain
    from a polymer is sampled, the center of the masked region will be forced to be in a binding site.
    &#34;&#34;&#34;

    def __init__(
        self,
        dataset,
        lower_limit=15,
        upper_limit=100,
        mask_residues=True,
        mask_whole_chains=False,
        mask_frac=None,
        force_binding_sites_frac=0,
        shuffle_batches=True,
        *args,
        **kwargs,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        dataset : ProteinDataset
            a ProteinDataset instance
        lower_limit : int, default 15
            the minimum number of residues to mask
        upper_limit : int, default 100
            the maximum number of residues to mask
        mask_frac : float, optional
            if given, the `lower_limit` and `upper_limit` are ignored and the number of residues to mask is `mask_frac` times the length of the chain
        mask_whole_chains : bool, default False
            if `True`, `upper_limit`, `force_binding_sites` and `lower_limit` are ignored and the whole chain is masked instead
        force_binding_sites_frac : float, default 0
            if &gt; 0, in the fraction of cases where a chain from a polymer is sampled, the center of the masked region will be
            forced to be in a binding site
        shuffle_clusters : bool, default True
            if `True`, a new representative is randomly selected for each cluster at each epoch (if `clustering_dict_path` is given)
        shuffle_batches : bool, default True
            if `True`, the batches are shuffled at each epoch
        &#34;&#34;&#34;

        super().__init__(
            dataset,
            collate_fn=_PadCollate(
                mask_residues=mask_residues,
                mask_whole_chains=mask_whole_chains,
                mask_frac=mask_frac,
                lower_limit=lower_limit,
                upper_limit=upper_limit,
                force_binding_sites_frac=force_binding_sites_frac,
            ),
            shuffle=shuffle_batches,
            *args,
            **kwargs,
        )

    @staticmethod
    def from_args(
        dataset_folder,
        features_folder=&#34;./data/tmp/&#34;,
        clustering_dict_path=None,
        max_length=None,
        rewrite=False,
        use_fraction=1,
        load_to_ram=False,
        debug=False,
        interpolate=&#34;none&#34;,
        node_features_type=None,
        entry_type=&#34;biounit&#34;,  # biounit, chain, pair
        classes_to_exclude=None,
        lower_limit=15,
        upper_limit=100,
        mask_residues=True,
        mask_whole_chains=False,
        mask_frac=None,
        force_binding_sites_frac=0,
        shuffle_clusters=True,
        shuffle_batches=True,
        *args,
        **kwargs,
    ) -&gt; None:
        &#34;&#34;&#34;
        Creates a `ProteinLoader` instance with a `ProteinDataset` from the given arguments

        Parameters
        ----------
        dataset_folder : str
            the path to the folder with proteinflow format input files (assumes that files are named {biounit_id}.pickle)
        features_folder : str
            the path to the folder where the ProteinMPNN features will be saved
        clustering_dict_path : str, optional
            path to the pickled clustering dictionary (keys are cluster ids, values are (biounit id, chain id) tuples)
        max_length : int, optional
            entries with total length of chains larger than `max_length` will be disregarded
        rewrite : bool, default False
            if `False`, existing feature files are not overwritten
        use_fraction : float, default 1
            the fraction of the clusters to use (first N in alphabetic order)
        load_to_ram : bool, default False
            if `True`, the data will be stored in RAM (use with caution! if RAM isn&#39;t big enough the machine might crash)
        debug : bool, default False
            only process 1000 files
        interpolate : {&#34;none&#34;, &#34;only_middle&#34;, &#34;all&#34;}
            `&#34;none&#34;` for no interpolation, `&#34;only_middle&#34;` for only linear interpolation in the middle, `&#34;all&#34;` for linear interpolation + ends generation
        node_features_type : {&#34;dihedral&#34;, &#34;sidechain_orientation&#34;, &#34;chemical&#34;, &#34;secondary_structure&#34;, &#34;sidechain_coords&#34;, or combinations with &#34;+&#34;}, optional
            the type of node features, e.g. `&#34;dihedral&#34;` or `&#34;sidechain_orientation+chemical&#34;`
        entry_type : {&#34;biounit&#34;, &#34;chain&#34;, &#34;pair&#34;}
            the type of entries to generate (`&#34;biounit&#34;` for biounit-level, `&#34;chain&#34;` for chain-level, `&#34;pair&#34;` for chain-chain pairs)
        classes_to_exclude : list of str, optional
            a list of classes to exclude from the dataset (select from `&#34;single_chains&#34;`, `&#34;heteromers&#34;`, `&#34;homomers&#34;`)
        lower_limit : int, default 15
            the minimum number of residues to mask
        upper_limit : int, default 100
            the maximum number of residues to mask
        mask_frac : float, optional
            if given, the `lower_limit` and `upper_limit` are ignored and the number of residues to mask is `mask_frac` times the length of the chain
        mask_whole_chains : bool, default False
            if `True`, `upper_limit`, `force_binding_sites` and `lower_limit` are ignored and the whole chain is masked instead
        force_binding_sites_frac : float, default 0
            if &gt; 0, in the fraction of cases where a chain from a polymer is sampled, the center of the masked region will be
            forced to be in a binding site
        shuffle_clusters : bool, default True
            if `True`, a new representative is randomly selected for each cluster at each epoch (if `clustering_dict_path` is given)
        shuffle_batches : bool, default True
            if `True`, the batches are shuffled at each epoch
        &#34;&#34;&#34;

        dataset = ProteinDataset(
            dataset_folder=dataset_folder,
            features_folder=features_folder,
            clustering_dict_path=clustering_dict_path,
            max_length=max_length,
            rewrite=rewrite,
            use_fraction=use_fraction,
            load_to_ram=load_to_ram,
            debug=debug,
            interpolate=interpolate,
            node_features_type=node_features_type,
            entry_type=entry_type,
            classes_to_exclude=classes_to_exclude,
            shuffle_clusters=shuffle_clusters,
        )
        return ProteinLoader(
            dataset=dataset,
            lower_limit=lower_limit,
            upper_limit=upper_limit,
            mask_residues=mask_residues,
            mask_whole_chains=mask_whole_chains,
            mask_frac=mask_frac,
            force_binding_sites_frac=force_binding_sites_frac,
            shuffle_batches=shuffle_batches,
            *args,
            **kwargs,
        )


def sidechain_order():
    &#34;&#34;&#34;
    Get a dictionary of sidechain atom orders

    Returns
    -------
    order_dict : dict
        a dictionary where keys are 3-letter aminoacid codes and values are lists of atom names (in PDB format) that correspond to
        coordinates in the `&#39;crd_sc&#39;` array generated by the `run_processing` function
    &#34;&#34;&#34;

    return SIDECHAIN_ORDER


def get_error_summary(log_file, verbose=True):
    &#34;&#34;&#34;
    Get a dictionary where keys are recognized exception names and values are lists of PDB ids that caused the exceptions

    Parameters
    ----------
    log_file : str
        the log file path
    verbose : bool, default True
        if `True`, the statistics are written in the standard output

    Returns
    -------
    log_dict : dict
        a dictionary where keys are recognized exception names and values are lists of PDB ids that caused the exceptions
    &#34;&#34;&#34;

    stats = defaultdict(lambda: [])
    with open(log_file, &#34;r&#34;) as f:
        for line in f.readlines():
            if line.startswith(&#34;&lt;&lt;&lt;&#34;):
                stats[line.split(&#34;:&#34;)[0]].append(line.split(&#34;:&#34;)[-1].strip())
    if verbose:
        keys = sorted(stats.keys(), key=lambda x: len(stats[x]), reverse=True)
        for key in keys:
            print(f&#34;{key}: {len(stats[key])}&#34;)
        print(f&#34;Total exceptions: {sum([len(x) for x in stats.values()])}&#34;)
    return stats


def check_pdb_snapshots():
    &#34;&#34;&#34;
    Get a list of PDB snapshots available for downloading

    Returns
    -------
    snapshots : list
        a list of snapshot names
    &#34;&#34;&#34;

    folders = _s3list(
        boto3.resource(&#34;s3&#34;, config=Config(signature_version=UNSIGNED)).Bucket(
            &#34;pdbsnapshots&#34;
        ),
        &#34;&#34;,
        recursive=False,
        list_objs=False,
    )
    return [x.key.strip(&#34;/&#34;) for x in folders]


def check_download_tags():
    &#34;&#34;&#34;
    Get a list of tags available for downloading

    Returns
    -------
    tags : list
        a list of tag names
    &#34;&#34;&#34;

    folders = _s3list(
        boto3.resource(&#34;s3&#34;, config=Config(signature_version=UNSIGNED)).Bucket(
            &#34;proteinflow-datasets&#34;
        ),
        &#34;&#34;,
        recursive=False,
        list_objs=False,
    )
    return [x.key.strip(&#34;/&#34;) for x in folders]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="proteinflow.check_download_tags"><code class="name flex">
<span>def <span class="ident">check_download_tags</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Get a list of tags available for downloading</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>tags</code></strong> :&ensp;<code>list</code></dt>
<dd>a list of tag names</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_download_tags():
    &#34;&#34;&#34;
    Get a list of tags available for downloading

    Returns
    -------
    tags : list
        a list of tag names
    &#34;&#34;&#34;

    folders = _s3list(
        boto3.resource(&#34;s3&#34;, config=Config(signature_version=UNSIGNED)).Bucket(
            &#34;proteinflow-datasets&#34;
        ),
        &#34;&#34;,
        recursive=False,
        list_objs=False,
    )
    return [x.key.strip(&#34;/&#34;) for x in folders]</code></pre>
</details>
</dd>
<dt id="proteinflow.check_pdb_snapshots"><code class="name flex">
<span>def <span class="ident">check_pdb_snapshots</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Get a list of PDB snapshots available for downloading</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>snapshots</code></strong> :&ensp;<code>list</code></dt>
<dd>a list of snapshot names</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_pdb_snapshots():
    &#34;&#34;&#34;
    Get a list of PDB snapshots available for downloading

    Returns
    -------
    snapshots : list
        a list of snapshot names
    &#34;&#34;&#34;

    folders = _s3list(
        boto3.resource(&#34;s3&#34;, config=Config(signature_version=UNSIGNED)).Bucket(
            &#34;pdbsnapshots&#34;
        ),
        &#34;&#34;,
        recursive=False,
        list_objs=False,
    )
    return [x.key.strip(&#34;/&#34;) for x in folders]</code></pre>
</details>
</dd>
<dt id="proteinflow.download_data"><code class="name flex">
<span>def <span class="ident">download_data</span></span>(<span>tag, local_datasets_folder='./data', skip_splitting=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Download a pre-computed dataset with train/test/validation splits</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tag</code></strong> :&ensp;<code>str</code></dt>
<dd>the name of the dataset to load</dd>
<dt><strong><code>local_datasets_folder</code></strong> :&ensp;<code>str</code>, default <code>"./data"</code></dt>
<dd>the path to the folder that will store proteinflow datasets, logs and temporary files</dd>
<dt><strong><code>skip_splitting</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, skip the split dictionary creation and the file moving steps</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def download_data(tag, local_datasets_folder=&#34;./data&#34;, skip_splitting=False):
    &#34;&#34;&#34;
    Download a pre-computed dataset with train/test/validation splits

    Parameters
    ----------
    tag : str
        the name of the dataset to load
    local_datasets_folder : str, default &#34;./data&#34;
        the path to the folder that will store proteinflow datasets, logs and temporary files
    skip_splitting : bool, default False
        if `True`, skip the split dictionary creation and the file moving steps
    &#34;&#34;&#34;

    data_path = _download_dataset(tag, local_datasets_folder)
    if not skip_splitting:
        _split_data(data_path)</code></pre>
</details>
</dd>
<dt id="proteinflow.generate_data"><code class="name flex">
<span>def <span class="ident">generate_data</span></span>(<span>tag, local_datasets_folder='./data', min_length=30, max_length=10000, resolution_thr=3.5, missing_ends_thr=0.3, missing_middle_thr=0.1, not_filter_methods=False, not_remove_redundancies=False, skip_splitting=False, seq_identity_threshold=0.9, n=None, force=False, split_tolerance=0.2, test_split=0.05, valid_split=0.05, pdb_snapshot=None, load_live=False, min_seq_id=0.3)</span>
</code></dt>
<dd>
<div class="desc"><p>Download and parse PDB files that meet filtering criteria</p>
<p>The output files are pickled nested dictionaries where first-level keys are chain Ids and second-level keys are
the following:</p>
<ul>
<li><code>'crd_bb'</code>: a <code>numpy</code> array of shape <code>(L, 4, 3)</code> with backbone atom coordinates (N, C, CA, O),</li>
<li><code>'crd_sc'</code>: a <code>numpy</code> array of shape <code>(L, 10, 3)</code> with sidechain atom coordinates (in a fixed order, check <code><a title="proteinflow.sidechain_order" href="#proteinflow.sidechain_order">sidechain_order()</a></code>),</li>
<li><code>'msk'</code>: a <code>numpy</code> array of shape <code>(L,)</code> where ones correspond to residues with known coordinates and
zeros to missing values,</li>
<li><code>'seq'</code>: a string of length <code>L</code> with residue types.</li>
</ul>
<p>All errors including reasons for filtering a file out are logged in the log file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tag</code></strong> :&ensp;<code>str</code></dt>
<dd>the name of the dataset to load</dd>
<dt><strong><code>local_datasets_folder</code></strong> :&ensp;<code>str</code>, default <code>"./data"</code></dt>
<dd>the path to the folder that will store proteinflow datasets, logs and temporary files</dd>
<dt><strong><code>min_length</code></strong> :&ensp;<code>int</code>, default <code>30</code></dt>
<dd>The minimum number of non-missing residues per chain</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>int</code>, default <code>10000</code></dt>
<dd>The maximum number of residues per chain (set None for no threshold)</dd>
<dt><strong><code>resolution_thr</code></strong> :&ensp;<code>float</code>, default <code>3.5</code></dt>
<dd>The maximum resolution</dd>
<dt><strong><code>missing_ends_thr</code></strong> :&ensp;<code>float</code>, default <code>0.3</code></dt>
<dd>The maximum fraction of missing residues at the ends</dd>
<dt><strong><code>missing_middle_thr</code></strong> :&ensp;<code>float</code>, default <code>0.1</code></dt>
<dd>The maximum fraction of missing values in the middle (after missing ends are disregarded)</dd>
<dt><strong><code>not_filter_methods</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>If <code>False</code>, only files obtained with X-ray or EM will be processed</dd>
<dt><strong><code>not_remove_redundancies</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>If 'False', removes biounits that are doubles of others sequence wise</dd>
<dt><strong><code>skip_splitting</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, skip the split dictionary creation and the file moving steps</dd>
<dt><strong><code>seq_identity_threshold</code></strong> :&ensp;<code>float</code>, default <code>0.9</code></dt>
<dd>The threshold upon which sequences are considered as one and the same (default: 90%)</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code>, default <code>None</code></dt>
<dd>The number of files to process (for debugging purposes)</dd>
<dt><strong><code>force</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>When <code>True</code>, rewrite the files if they already exist</dd>
<dt><strong><code>split_tolerance</code></strong> :&ensp;<code>float</code>, default <code>0.2</code></dt>
<dd>The tolerance on the split ratio (default 20%)</dd>
<dt><strong><code>test_split</code></strong> :&ensp;<code>float</code>, default <code>0.05</code></dt>
<dd>The percentage of chains to put in the test set (default 5%)</dd>
<dt><strong><code>valid_split</code></strong> :&ensp;<code>float</code>, default <code>0.05</code></dt>
<dd>The percentage of chains to put in the validation set (default 5%)</dd>
<dt><strong><code>pdb_snapshot</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the PDB snapshot to use, by default the latest is used</dd>
<dt><strong><code>load_live</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, load the files that are not in the latest PDB snapshot from the PDB FTP server (forced to <code>False</code> if <code>pdb_snapshot</code> is not <code>None</code>)</dd>
<dt><strong><code>min_seq_id</code></strong> :&ensp;<code>float in [0, 1]</code>, default <code>0.3</code></dt>
<dd>minimum sequence identity for <code>mmseqs</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>dict</code></dt>
<dd>a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_data(
    tag,
    local_datasets_folder=&#34;./data&#34;,
    min_length=30,
    max_length=10000,
    resolution_thr=3.5,
    missing_ends_thr=0.3,
    missing_middle_thr=0.1,
    not_filter_methods=False,
    not_remove_redundancies=False,
    skip_splitting=False,
    seq_identity_threshold=0.9,
    n=None,
    force=False,
    split_tolerance=0.2,
    test_split=0.05,
    valid_split=0.05,
    pdb_snapshot=None,
    load_live=False,
    min_seq_id=0.3,
):
    &#34;&#34;&#34;
    Download and parse PDB files that meet filtering criteria

    The output files are pickled nested dictionaries where first-level keys are chain Ids and second-level keys are
    the following:

    - `&#39;crd_bb&#39;`: a `numpy` array of shape `(L, 4, 3)` with backbone atom coordinates (N, C, CA, O),
    - `&#39;crd_sc&#39;`: a `numpy` array of shape `(L, 10, 3)` with sidechain atom coordinates (in a fixed order, check `sidechain_order()`),
    - `&#39;msk&#39;`: a `numpy` array of shape `(L,)` where ones correspond to residues with known coordinates and
        zeros to missing values,
    - `&#39;seq&#39;`: a string of length `L` with residue types.

    All errors including reasons for filtering a file out are logged in the log file.

    Parameters
    ----------
    tag : str
        the name of the dataset to load
    local_datasets_folder : str, default &#34;./data&#34;
        the path to the folder that will store proteinflow datasets, logs and temporary files
    min_length : int, default 30
        The minimum number of non-missing residues per chain
    max_length : int, default 10000
        The maximum number of residues per chain (set None for no threshold)
    resolution_thr : float, default 3.5
        The maximum resolution
    missing_ends_thr : float, default 0.3
        The maximum fraction of missing residues at the ends
    missing_middle_thr : float, default 0.1
        The maximum fraction of missing values in the middle (after missing ends are disregarded)
    not_filter_methods : bool, default False
        If `False`, only files obtained with X-ray or EM will be processed
    not_remove_redundancies : bool, default False
        If &#39;False&#39;, removes biounits that are doubles of others sequence wise
    skip_splitting : bool, default False
        if `True`, skip the split dictionary creation and the file moving steps
    seq_identity_threshold : float, default 0.9
        The threshold upon which sequences are considered as one and the same (default: 90%)
    n : int, default None
        The number of files to process (for debugging purposes)
    force : bool, default False
        When `True`, rewrite the files if they already exist
    split_tolerance : float, default 0.2
        The tolerance on the split ratio (default 20%)
    test_split : float, default 0.05
        The percentage of chains to put in the test set (default 5%)
    valid_split : float, default 0.05
        The percentage of chains to put in the validation set (default 5%)
    pdb_snapshot : str, optional
        the PDB snapshot to use, by default the latest is used
    load_live : bool, default False
        if `True`, load the files that are not in the latest PDB snapshot from the PDB FTP server (forced to `False` if `pdb_snapshot` is not `None`)
    min_seq_id : float in [0, 1], default 0.3
        minimum sequence identity for `mmseqs`

    Returns
    -------
    log : dict
        a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors

    &#34;&#34;&#34;
    _check_mmseqs()
    filter_methods = not not_filter_methods
    remove_redundancies = not not_remove_redundancies
    tmp_folder = os.path.join(local_datasets_folder, &#34;tmp&#34;)
    output_folder = os.path.join(local_datasets_folder, f&#34;proteinflow_{tag}&#34;)
    log_folder = os.path.join(local_datasets_folder, &#34;logs&#34;)
    out_split_dict_folder = os.path.join(output_folder, &#34;splits_dict&#34;)

    log_dict = _run_processing(
        tmp_folder=tmp_folder,
        output_folder=output_folder,
        log_folder=log_folder,
        min_length=min_length,
        max_length=max_length,
        resolution_thr=resolution_thr,
        missing_ends_thr=missing_ends_thr,
        missing_middle_thr=missing_middle_thr,
        filter_methods=filter_methods,
        remove_redundancies=remove_redundancies,
        seq_identity_threshold=seq_identity_threshold,
        n=n,
        force=force,
        tag=tag,
        pdb_snapshot=pdb_snapshot,
        load_live=load_live,
    )
    if not skip_splitting:
        _get_split_dictionaries(
            tmp_folder=tmp_folder,
            output_folder=output_folder,
            split_tolerance=split_tolerance,
            test_split=test_split,
            valid_split=valid_split,
            out_split_dict_folder=out_split_dict_folder,
            min_seq_id=min_seq_id,
        )

        _split_data(output_folder)
    return log_dict</code></pre>
</details>
</dd>
<dt id="proteinflow.get_error_summary"><code class="name flex">
<span>def <span class="ident">get_error_summary</span></span>(<span>log_file, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Get a dictionary where keys are recognized exception names and values are lists of PDB ids that caused the exceptions</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>log_file</code></strong> :&ensp;<code>str</code></dt>
<dd>the log file path</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>if <code>True</code>, the statistics are written in the standard output</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>log_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>a dictionary where keys are recognized exception names and values are lists of PDB ids that caused the exceptions</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_error_summary(log_file, verbose=True):
    &#34;&#34;&#34;
    Get a dictionary where keys are recognized exception names and values are lists of PDB ids that caused the exceptions

    Parameters
    ----------
    log_file : str
        the log file path
    verbose : bool, default True
        if `True`, the statistics are written in the standard output

    Returns
    -------
    log_dict : dict
        a dictionary where keys are recognized exception names and values are lists of PDB ids that caused the exceptions
    &#34;&#34;&#34;

    stats = defaultdict(lambda: [])
    with open(log_file, &#34;r&#34;) as f:
        for line in f.readlines():
            if line.startswith(&#34;&lt;&lt;&lt;&#34;):
                stats[line.split(&#34;:&#34;)[0]].append(line.split(&#34;:&#34;)[-1].strip())
    if verbose:
        keys = sorted(stats.keys(), key=lambda x: len(stats[x]), reverse=True)
        for key in keys:
            print(f&#34;{key}: {len(stats[key])}&#34;)
        print(f&#34;Total exceptions: {sum([len(x) for x in stats.values()])}&#34;)
    return stats</code></pre>
</details>
</dd>
<dt id="proteinflow.sidechain_order"><code class="name flex">
<span>def <span class="ident">sidechain_order</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Get a dictionary of sidechain atom orders</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>order_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>a dictionary where keys are 3-letter aminoacid codes and values are lists of atom names (in PDB format) that correspond to
coordinates in the <code>'crd_sc'</code> array generated by the <code>run_processing</code> function</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sidechain_order():
    &#34;&#34;&#34;
    Get a dictionary of sidechain atom orders

    Returns
    -------
    order_dict : dict
        a dictionary where keys are 3-letter aminoacid codes and values are lists of atom names (in PDB format) that correspond to
        coordinates in the `&#39;crd_sc&#39;` array generated by the `run_processing` function
    &#34;&#34;&#34;

    return SIDECHAIN_ORDER</code></pre>
</details>
</dd>
<dt id="proteinflow.split_data"><code class="name flex">
<span>def <span class="ident">split_data</span></span>(<span>tag, local_datasets_folder='./data', split_tolerance=0.2, test_split=0.05, valid_split=0.05, ignore_existing=False, min_seq_id=0.3)</span>
</code></dt>
<dd>
<div class="desc"><p>Split <code><a title="proteinflow" href="#proteinflow">proteinflow</a></code> entry files into training, test and validation.</p>
<p>Our splitting algorithm has two objectives: achieving minimal data leakage and balancing the proportion of
single chain, homomer and heteromer entries.</p>
<p>It follows these steps:</p>
<ol>
<li>cluster chains by sequence identity,</li>
<li>generate a graph where nodes are the clusters and edges are protein-protein interactions between chains
from those clusters,</li>
<li>split connected components of the graph into training, test and validation subsets while keeping the proportion
of single chains, homomers and heteromers close to that in the full dataset (within <code>split_tolerance</code>).</li>
</ol>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tag</code></strong> :&ensp;<code>str</code></dt>
<dd>the name of the dataset to load</dd>
<dt><strong><code>local_datasets_folder</code></strong> :&ensp;<code>str</code>, default <code>"./data"</code></dt>
<dd>the path to the folder that will store proteinflow datasets, logs and temporary files</dd>
<dt><strong><code>split_tolerance</code></strong> :&ensp;<code>float</code>, default <code>0.2</code></dt>
<dd>The tolerance on the split ratio (default 20%)</dd>
<dt><strong><code>test_split</code></strong> :&ensp;<code>float</code>, default <code>0.05</code></dt>
<dd>The percentage of chains to put in the test set (default 5%)</dd>
<dt><strong><code>valid_split</code></strong> :&ensp;<code>float</code>, default <code>0.05</code></dt>
<dd>The percentage of chains to put in the validation set (default 5%)</dd>
<dt><strong><code>ignore_existing</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>If <code>True</code>, overwrite existing dictionaries for this tag; otherwise, load the existing dictionary</dd>
<dt><strong><code>min_seq_id</code></strong> :&ensp;<code>float in [0, 1]</code>, default <code>0.3</code></dt>
<dd>minimum sequence identity for <code>mmseqs</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>dict</code></dt>
<dd>a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_data(
    tag,
    local_datasets_folder=&#34;./data&#34;,
    split_tolerance=0.2,
    test_split=0.05,
    valid_split=0.05,
    ignore_existing=False,
    min_seq_id=0.3,
):
    &#34;&#34;&#34;
    Split `proteinflow` entry files into training, test and validation.

    Our splitting algorithm has two objectives: achieving minimal data leakage and balancing the proportion of
    single chain, homomer and heteromer entries.

    It follows these steps:

    1. cluster chains by sequence identity,
    2. generate a graph where nodes are the clusters and edges are protein-protein interactions between chains
    from those clusters,
    3. split connected components of the graph into training, test and validation subsets while keeping the proportion
    of single chains, homomers and heteromers close to that in the full dataset (within `split_tolerance`).


    Parameters
    ----------
    tag : str
        the name of the dataset to load
    local_datasets_folder : str, default &#34;./data&#34;
        the path to the folder that will store proteinflow datasets, logs and temporary files
    split_tolerance : float, default 0.2
        The tolerance on the split ratio (default 20%)
    test_split : float, default 0.05
        The percentage of chains to put in the test set (default 5%)
    valid_split : float, default 0.05
        The percentage of chains to put in the validation set (default 5%)
    ignore_existing : bool, default False
        If `True`, overwrite existing dictionaries for this tag; otherwise, load the existing dictionary
    min_seq_id : float in [0, 1], default 0.3
        minimum sequence identity for `mmseqs`

    Returns
    -------
    log : dict
        a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors
    &#34;&#34;&#34;

    _check_mmseqs()
    tmp_folder = os.path.join(local_datasets_folder, &#34;tmp&#34;)
    output_folder = os.path.join(local_datasets_folder, f&#34;proteinflow_{tag}&#34;)
    out_split_dict_folder = os.path.join(output_folder, &#34;splits_dict&#34;)
    exists = False

    if os.path.exists(out_split_dict_folder):
        if not ignore_existing:
            warnings.warn(
                f&#34;Found an existing dictionary for tag {tag}. proteinflow will load it and ignore the parameters! Run with --ignore_existing to overwrite.&#34;
            )
            exists = True
    if not exists:
        _get_split_dictionaries(
            tmp_folder=tmp_folder,
            output_folder=output_folder,
            split_tolerance=split_tolerance,
            test_split=test_split,
            valid_split=valid_split,
            out_split_dict_folder=out_split_dict_folder,
            min_seq_id=min_seq_id,
        )

    _split_data(output_folder)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="proteinflow.ProteinDataset"><code class="flex name class">
<span>class <span class="ident">ProteinDataset</span></span>
<span>(</span><span>dataset_folder, features_folder='./data/tmp/', clustering_dict_path=None, max_length=None, rewrite=False, use_fraction=1, load_to_ram=False, debug=False, interpolate='none', node_features_type='zeros', debug_file_path=None, entry_type='biounit', classes_to_exclude=None, shuffle_clusters=True, feature_functions=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Dataset to load proteinflow data</p>
<p>Saves the model input tensors as pickle files in <code>features_folder</code>. When <code>clustering_dict_path</code> is provided,
at each iteration a random bionit from a cluster is sampled.</p>
<p>If a complex contains multiple chains, they are concatenated. The sequence identity information is preserved in the
<code>'chain_encoding_all'</code> object and in the <code>'residue_idx'</code> arrays the chain change is denoted by a +100 jump.</p>
<p>Returns dictionaries with the following keys and values (all values are <code>torch</code> tensors):</p>
<ul>
<li><code>'X'</code>: 3D coordinates of N, C, Ca, O, <code>(total_L, 4, 3)</code>,</li>
<li><code>'S'</code>: sequence indices (shape <code>(total_L)</code>),</li>
<li><code>'mask'</code>: residue mask (0 where coordinates are missing, 1 otherwise; with interpolation 0s are replaced with 1s), <code>(total_L)</code>,</li>
<li><code>'mask_original'</code>: residue mask (0 where coordinates are missing, 1 otherwise; not changed with interpolation), <code>(total_L)</code>,</li>
<li><code>'residue_idx'</code>: residue indices (from 0 to length of sequence, +100 where chains change), <code>(total_L)</code>,</li>
<li><code>'chain_encoding_all'</code>: chain indices, <code>(total_L)</code>,</li>
<li><code>'chain_id</code>': a sampled chain index,</li>
<li><code>'chain_dict'</code>: a dictionary of chain ids (keys are chain ids, e.g. <code>'A'</code>, values are the indices used in <code>'chain_id'</code> and <code>'chain_encoding_all'</code> objects)</li>
</ul>
<p>You can also choose to include additional features (set in the <code>node_features_type</code> parameter):</p>
<ul>
<li><code>'sidechain_orientation'</code>: a unit vector in the direction of the sidechain, <code>(total_L, 3)</code>,</li>
<li><code>'dihedral'</code>: the dihedral angles, <code>(total_L, 2)</code>,</li>
<li><code>'chemical'</code>: hydropathy, volume, charge, polarity, acceptor/donor features, <code>(total_L, 6)</code>,</li>
<li><code>'secondary_structure'</code>: a one-hot encoding of secondary structure ([alpha-helix, beta-sheet, coil]), <code>(total_L, 3)</code>,</li>
<li><code>'sidechain_coords'</code>: the coordinates of the sidechain atoms (see <code><a title="proteinflow.sidechain_order" href="#proteinflow.sidechain_order">sidechain_order()</a></code> for the order), <code>(total_L, 10, 3)</code>,</li>
</ul>
<p>In order to compute additional features, use the <code>feature_functions</code> parameter. It should be a dictionary with keys
corresponding to the feature names and values corresponding to the functions that compute the features. The functions
should take a chain dictionary and an integer representation of the sequence as input (the dictionary is in <code><a title="proteinflow" href="#proteinflow">proteinflow</a></code> format,
see the docs for <code><a title="proteinflow.generate_data" href="#proteinflow.generate_data">generate_data()</a></code> for details) and return a <code>numpy</code> array shaped as <code>(#residues, #features)</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>the path to the folder with proteinflow format input files (assumes that files are named {biounit_id}.pickle)</dd>
<dt><strong><code>features_folder</code></strong> :&ensp;<code>str</code>, default <code>"./data/tmp/"</code></dt>
<dd>the path to the folder where the ProteinMPNN features will be saved</dd>
<dt><strong><code>clustering_dict_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>path to the pickled clustering dictionary (keys are cluster ids, values are (biounit id, chain id) tuples)</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>entries with total length of chains larger than <code>max_length</code> will be disregarded</dd>
<dt><strong><code>rewrite</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>False</code>, existing feature files are not overwritten</dd>
<dt><strong><code>use_fraction</code></strong> :&ensp;<code>float</code>, default <code>1</code></dt>
<dd>the fraction of the clusters to use (first N in alphabetic order)</dd>
<dt><strong><code>load_to_ram</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, the data will be stored in RAM (use with caution! if RAM isn't big enough the machine might crash)</dd>
<dt><strong><code>debug</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>only process 1000 files</dd>
<dt><strong><code>interpolate</code></strong> :&ensp;<code>{"none", "only_middle", "all"}</code></dt>
<dd><code>"none"</code> for no interpolation, <code>"only_middle"</code> for only linear interpolation in the middle, <code>"all"</code> for linear interpolation + ends generation</dd>
<dt><strong><code>node_features_type</code></strong> :&ensp;<code>{"zeros", "dihedral", "sidechain_orientation", "chemical", "secondary_structure"</code> or <code>combinations with "+"}</code></dt>
<dd>the type of node features, e.g. <code>"dihedral"</code> or <code>"sidechain_orientation+chemical"</code></dd>
<dt><strong><code>debug_file_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>if not <code>None</code>, open this single file instead of loading the dataset</dd>
<dt><strong><code>entry_type</code></strong> :&ensp;<code>{"biounit", "chain", "pair"}</code></dt>
<dd>the type of entries to generate (<code>"biounit"</code> for biounit-level complexes, <code>"chain"</code> for chain-level, <code>"pair"</code>
for chain-chain pairs (all pairs that are seen in the same biounit and have intersecting coordinate clouds))</dd>
<dt><strong><code>classes_to_exclude</code></strong> :&ensp;<code>list</code> of <code>str</code>, optional</dt>
<dd>a list of classes to exclude from the dataset (select from <code>"single_chains"</code>, <code>"heteromers"</code>, <code>"homomers"</code>)</dd>
<dt><strong><code>shuffle_clusters</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>if <code>True</code>, a new representative is randomly selected for each cluster at each epoch (if <code>clustering_dict_path</code> is given)</dd>
<dt><strong><code>feature_functions</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>a dictionary of functions to compute additional features (keys are the names of the features, values are the functions)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ProteinDataset(Dataset):
    &#34;&#34;&#34;
    Dataset to load proteinflow data

    Saves the model input tensors as pickle files in `features_folder`. When `clustering_dict_path` is provided,
    at each iteration a random bionit from a cluster is sampled.

    If a complex contains multiple chains, they are concatenated. The sequence identity information is preserved in the
    `&#39;chain_encoding_all&#39;` object and in the `&#39;residue_idx&#39;` arrays the chain change is denoted by a +100 jump.

    Returns dictionaries with the following keys and values (all values are `torch` tensors):

    - `&#39;X&#39;`: 3D coordinates of N, C, Ca, O, `(total_L, 4, 3)`,
    - `&#39;S&#39;`: sequence indices (shape `(total_L)`),
    - `&#39;mask&#39;`: residue mask (0 where coordinates are missing, 1 otherwise; with interpolation 0s are replaced with 1s), `(total_L)`,
    - `&#39;mask_original&#39;`: residue mask (0 where coordinates are missing, 1 otherwise; not changed with interpolation), `(total_L)`,
    - `&#39;residue_idx&#39;`: residue indices (from 0 to length of sequence, +100 where chains change), `(total_L)`,
    - `&#39;chain_encoding_all&#39;`: chain indices, `(total_L)`,
    - `&#39;chain_id`&#39;: a sampled chain index,
    - `&#39;chain_dict&#39;`: a dictionary of chain ids (keys are chain ids, e.g. `&#39;A&#39;`, values are the indices used in `&#39;chain_id&#39;` and `&#39;chain_encoding_all&#39;` objects)

    You can also choose to include additional features (set in the `node_features_type` parameter):

    - `&#39;sidechain_orientation&#39;`: a unit vector in the direction of the sidechain, `(total_L, 3)`,
    - `&#39;dihedral&#39;`: the dihedral angles, `(total_L, 2)`,
    - `&#39;chemical&#39;`: hydropathy, volume, charge, polarity, acceptor/donor features, `(total_L, 6)`,
    - `&#39;secondary_structure&#39;`: a one-hot encoding of secondary structure ([alpha-helix, beta-sheet, coil]), `(total_L, 3)`,
    - `&#39;sidechain_coords&#39;`: the coordinates of the sidechain atoms (see `proteinflow.sidechain_order()` for the order), `(total_L, 10, 3)`,

    In order to compute additional features, use the `feature_functions` parameter. It should be a dictionary with keys
    corresponding to the feature names and values corresponding to the functions that compute the features. The functions
    should take a chain dictionary and an integer representation of the sequence as input (the dictionary is in `proteinflow` format,
    see the docs for `generate_data` for details) and return a `numpy` array shaped as `(#residues, #features)`.

    &#34;&#34;&#34;

    def __init__(
        self,
        dataset_folder,
        features_folder=&#34;./data/tmp/&#34;,
        clustering_dict_path=None,
        max_length=None,
        rewrite=False,
        use_fraction=1,
        load_to_ram=False,
        debug=False,
        interpolate=&#34;none&#34;,
        node_features_type=&#34;zeros&#34;,
        debug_file_path=None,
        entry_type=&#34;biounit&#34;,  # biounit, chain, pair
        classes_to_exclude=None,  # heteromers, homomers, single_chains
        shuffle_clusters=True,
        feature_functions=None,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        dataset_folder : str
            the path to the folder with proteinflow format input files (assumes that files are named {biounit_id}.pickle)
        features_folder : str, default &#34;./data/tmp/&#34;
            the path to the folder where the ProteinMPNN features will be saved
        clustering_dict_path : str, optional
            path to the pickled clustering dictionary (keys are cluster ids, values are (biounit id, chain id) tuples)
        max_length : int, optional
            entries with total length of chains larger than `max_length` will be disregarded
        rewrite : bool, default False
            if `False`, existing feature files are not overwritten
        use_fraction : float, default 1
            the fraction of the clusters to use (first N in alphabetic order)
        load_to_ram : bool, default False
            if `True`, the data will be stored in RAM (use with caution! if RAM isn&#39;t big enough the machine might crash)
        debug : bool, default False
            only process 1000 files
        interpolate : {&#34;none&#34;, &#34;only_middle&#34;, &#34;all&#34;}
            `&#34;none&#34;` for no interpolation, `&#34;only_middle&#34;` for only linear interpolation in the middle, `&#34;all&#34;` for linear interpolation + ends generation
        node_features_type : {&#34;zeros&#34;, &#34;dihedral&#34;, &#34;sidechain_orientation&#34;, &#34;chemical&#34;, &#34;secondary_structure&#34; or combinations with &#34;+&#34;}
            the type of node features, e.g. `&#34;dihedral&#34;` or `&#34;sidechain_orientation+chemical&#34;`
        debug_file_path : str, optional
            if not `None`, open this single file instead of loading the dataset
        entry_type : {&#34;biounit&#34;, &#34;chain&#34;, &#34;pair&#34;}
            the type of entries to generate (`&#34;biounit&#34;` for biounit-level complexes, `&#34;chain&#34;` for chain-level, `&#34;pair&#34;`
            for chain-chain pairs (all pairs that are seen in the same biounit and have intersecting coordinate clouds))
        classes_to_exclude : list of str, optional
            a list of classes to exclude from the dataset (select from `&#34;single_chains&#34;`, `&#34;heteromers&#34;`, `&#34;homomers&#34;`)
        shuffle_clusters : bool, default True
            if `True`, a new representative is randomly selected for each cluster at each epoch (if `clustering_dict_path` is given)
        feature_functions : dict, optional
            a dictionary of functions to compute additional features (keys are the names of the features, values are the functions)
        &#34;&#34;&#34;

        alphabet = ALPHABET
        self.alphabet_dict = defaultdict(lambda: 0)
        for i, letter in enumerate(alphabet):
            self.alphabet_dict[letter] = i
        self.alphabet_dict[&#34;X&#34;] = 0
        self.files = defaultdict(lambda: defaultdict(list))  # file path by biounit id
        self.loaded = None
        self.dataset_folder = dataset_folder
        self.features_folder = features_folder
        self.feature_types = []
        if node_features_type is not None:
            self.feature_types = node_features_type.split(&#34;+&#34;)
        self.entry_type = entry_type
        self.shuffle_clusters = shuffle_clusters
        self.feature_functions = {
            &#34;sidechain_orientation&#34;: self._sidechain,
            &#34;dihedral&#34;: self._dihedral,
            &#34;chemical&#34;: self._chemical,
            &#34;secondary_structure&#34;: self._sse,
            &#34;sidechain_coords&#34;: self._sidechain_coords,
        }
        self.feature_functions.update(feature_functions or {})
        if classes_to_exclude is not None and not all(
            [
                x in [&#34;single_chains&#34;, &#34;heteromers&#34;, &#34;homomers&#34;]
                for x in classes_to_exclude
            ]
        ):
            raise ValueError(
                &#34;Invalid class to exclude, choose from &#39;single_chains&#39;, &#39;heteromers&#39;, &#39;homomers&#39;&#34;
            )

        if debug_file_path is not None:
            self.dataset_folder = os.path.dirname(debug_file_path)
            debug_file_path = os.path.basename(debug_file_path)

        self.main_atom_dict = defaultdict(lambda: None)
        d1to3 = {v: k for k, v in D3TO1.items()}
        for i, letter in enumerate(alphabet):
            if i == 0:
                continue
            self.main_atom_dict[i] = MAIN_ATOMS[d1to3[letter]]

        # create feature folder if it does not exist
        if not os.path.exists(self.features_folder):
            os.makedirs(self.features_folder)

        self.interpolate = interpolate
        # generate the feature files
        print(&#34;Processing files...&#34;)
        if debug_file_path is None:
            to_process = os.listdir(dataset_folder)
        else:
            to_process = [debug_file_path]
        if clustering_dict_path is not None and use_fraction &lt; 1:
            with open(clustering_dict_path, &#34;rb&#34;) as f:
                clusters = pickle.load(f)
            keys = sorted(clusters.keys())[: int(len(clusters) * use_fraction)]
            to_process = set()
            for key in keys:
                to_process.update([x[0] for x in clusters[key]])
            to_process = list(to_process)
        if debug:
            to_process = to_process[:1000]
        # output_tuples = [self._process(x, rewrite=rewrite) for x in tqdm(to_process)]
        if self.entry_type == &#34;pair&#34;:
            print(
                &#34;Please note that the pair entry type takes longer to process than the other two. The progress bar is not linear because of the varying number of chains per file.&#34;
            )
        output_tuples_list = p_map(
            lambda x: self._process(x, rewrite=rewrite, max_length=max_length),
            to_process,
        )
        # save the file names
        for output_tuples in output_tuples_list:
            for id, filename, chain_set in output_tuples:
                for chain in chain_set:
                    self.files[id][chain].append(filename)
        # filter by length
        # seen = set()
        # if max_length is not None:
        #     to_remove = []
        #     for id, chain_dict in self.files.items():
        #         for chain, file_list in chain_dict.items():
        #             for file in file_list:
        #                 if file in seen:
        #                     continue
        #                 seen.add(file)
        #                 with open(file, &#34;rb&#34;) as f:
        #                     data = pickle.load(f)
        #                     if len(data[&#34;S&#34;]) &gt; max_length:
        #                         to_remove.append(file)
        #     for id in list(self.files.keys()):
        #         chain_dict = self.files[id]
        #         for chain in list(chain_dict.keys()):
        #             file_list = chain_dict[chain]
        #             for file in file_list:
        #                 if file in to_remove:
        #                     self.files[id][chain].remove(file)
        #                     if len(self.files[id][chain]) == 0:
        #                         self.files[id].pop(chain)
        #                     if len(self.files[id]) == 0:
        #                         self.files.pop(id)
        # load the clusters
        if classes_to_exclude is None:
            classes_to_exclude = []
        elif clustering_dict_path is None:
            raise ValueError(
                &#34;classes_to_exclude is not None, but clustering_dict_path is None&#34;
            )
        if clustering_dict_path is not None:
            if entry_type == &#34;pair&#34;:
                classes_to_exclude = set(classes_to_exclude)
                classes_to_exclude.add(&#34;single_chains&#34;)
                classes_to_exclude = list(classes_to_exclude)
            with open(clustering_dict_path, &#34;rb&#34;) as f:
                self.clusters = pickle.load(f)  # list of biounit ids by cluster id
                classes = pickle.load(f)
            to_exclude = set()
            for c in classes_to_exclude:
                for key, id_arr in classes.get(c, {}).items():
                    for id, _ in id_arr:
                        to_exclude.add(id)
            for key in list(self.clusters.keys()):
                cluster_list = []
                for x in self.clusters[key]:
                    if x[0] in to_exclude:
                        continue
                    id = x[0].split(&#34;.&#34;)[0]
                    chain = x[1]
                    if id not in self.files:
                        continue
                    if chain not in self.files[id]:
                        continue
                    if len(self.files[id][chain]) == 0:
                        continue
                    cluster_list.append([id, chain])
                self.clusters[key] = cluster_list
                if len(self.clusters[key]) == 0:
                    self.clusters.pop(key)
            self.data = list(self.clusters.keys())
        else:
            self.clusters = None
            self.data = list(self.files.keys())
        # create a smaller dataset if necessary
        if clustering_dict_path is None and use_fraction &lt; 1:
            self.data = sorted(self.data)[: int(len(self.data) * use_fraction)]
        if load_to_ram:
            print(&#34;Loading to RAM...&#34;)
            self.loaded = {}
            seen = set()
            for id in self.files:
                for chain, file_list in self.files[id].items():
                    for file in file_list:
                        if file in seen:
                            continue
                        seen.add(file)
                        with open(file, &#34;rb&#34;) as f:
                            self.loaded[file] = pickle.load(f)

    def _interpolate(self, crd_i, mask_i):
        &#34;&#34;&#34;
        Fill in missing values in the middle with linear interpolation and (if fill_ends is true) build an initialization for the ends

        For the ends, the first 10 residues are 3.6 A apart from each other on a straight line from the last known value away from the center.
        Next they are 3.6 A apart in a random direction.
        &#34;&#34;&#34;

        if self.interpolate in [&#34;all&#34;, &#34;only_middle&#34;]:
            crd_i[(1 - mask_i).astype(bool)] = np.nan
            df = pd.DataFrame(crd_i.reshape((crd_i.shape[0], -1)))
            crd_i = df.interpolate(limit_area=&#34;inside&#34;).values.reshape(crd_i.shape)
        if self.interpolate == &#34;all&#34;:
            non_nans = np.where(~np.isnan(crd_i[:, 0, 0]))[0]
            known_start = non_nans[0]
            known_end = non_nans[-1] + 1
            if known_end &lt; len(crd_i) or known_start &gt; 0:
                center = crd_i[non_nans, 2, :].mean(0)
                if known_start &gt; 0:
                    direction = crd_i[known_start, 2, :] - center
                    direction = direction / linalg.norm(direction)
                    for i in range(0, min(known_start, 10)):
                        crd_i[known_start - i - 1] = (
                            crd_i[known_start - i] + direction * 3.6
                        )
                    for i in range(min(known_start, 10), known_start):
                        v = np.random.rand(3)
                        v = v / linalg.norm(v)
                        crd_i[known_start - i - 1] = crd_i[known_start - i] + v * 3.6
                if known_end &lt; len(crd_i):
                    to_add = len(crd_i) - known_end
                    direction = crd_i[known_end - 1, 2, :] - center
                    direction = direction / linalg.norm(direction)
                    for i in range(0, min(to_add, 10)):
                        crd_i[known_end + i] = (
                            crd_i[known_end + i - 1] + direction * 3.6
                        )
                    for i in range(min(to_add, 10), to_add):
                        v = np.random.rand(3)
                        v = v / linalg.norm(v)
                        crd_i[known_end + i] = crd_i[known_end + i - 1] + v * 3.6
            mask_i = np.ones(mask_i.shape)
        if self.interpolate in [&#34;only_middle&#34;]:
            nan_mask = np.isnan(crd_i)  # in the middle the nans have been interpolated
            mask_i[~np.isnan(crd_i[:, 0, 0])] = 1
            crd_i[nan_mask] = 0
        if self.interpolate == &#34;zeros&#34;:
            non_nans = np.where(mask_i != 0)[0]
            known_start = non_nans[0]
            known_end = non_nans[-1] + 1
            mask_i[known_start:known_end] = 1
        return crd_i, mask_i

    def _dihedral_angle(self, crd, msk):
        &#34;&#34;&#34;Praxeolitic formula
        1 sqrt, 1 cross product&#34;&#34;&#34;

        p0 = crd[..., 0, :]
        p1 = crd[..., 1, :]
        p2 = crd[..., 2, :]
        p3 = crd[..., 3, :]

        b0 = -1.0 * (p1 - p0)
        b1 = p2 - p1
        b2 = p3 - p2

        b1 /= np.expand_dims(np.linalg.norm(b1, axis=-1), -1) + 1e-7

        v = b0 - np.expand_dims(np.einsum(&#34;bi,bi-&gt;b&#34;, b0, b1), -1) * b1
        w = b2 - np.expand_dims(np.einsum(&#34;bi,bi-&gt;b&#34;, b2, b1), -1) * b1

        x = np.einsum(&#34;bi,bi-&gt;b&#34;, v, w)
        y = np.einsum(&#34;bi,bi-&gt;b&#34;, np.cross(b1, v), w)
        dh = np.degrees(np.arctan2(y, x))
        dh[1 - msk] = 0
        return dh

    def _dihedral(self, chain_dict, seq):
        &#34;&#34;&#34;
        Dihedral angles
        &#34;&#34;&#34;

        crd = chain_dict[&#34;crd_bb&#34;]
        msk = chain_dict[&#34;msk&#34;]
        angles = []
        # N, C, Ca, O
        # psi
        p = crd[:-1, [0, 2, 1], :]
        p = np.concatenate([p, crd[1:, [0], :]], 1)
        p = np.pad(p, ((0, 1), (0, 0), (0, 0)))
        angles.append(self._dihedral_angle(p, msk))
        # phi
        p = crd[:-1, [1], :]
        p = np.concatenate([p, crd[1:, [0, 2, 1]]], 1)
        p = np.pad(p, ((1, 0), (0, 0), (0, 0)))
        angles.append(self._dihedral_angle(p, msk))
        angles = np.stack(angles, -1)
        return angles

    def _sidechain(self, chain_dict, seq):
        &#34;&#34;&#34;
        Sidechain orientation (defined by the &#39;main atoms&#39; in the `main_atom_dict` dictionary)
        &#34;&#34;&#34;

        crd_sc = chain_dict[&#34;crd_sc&#34;]
        crd_bb = chain_dict[&#34;crd_bb&#34;]
        orientation = np.zeros((crd_sc.shape[0], 3))
        for i in range(1, 21):
            if self.main_atom_dict[i] is not None:
                orientation[seq == i] = (
                    crd_sc[seq == i, self.main_atom_dict[i], :] - crd_bb[seq == i, 2, :]
                )
            else:
                S_mask = seq == i
                orientation[S_mask] = np.random.rand(*orientation[S_mask].shape)
        orientation /= np.expand_dims(linalg.norm(orientation, axis=-1), -1) + 1e-7
        return orientation

    def _chemical(self, chain_dict, seq):
        &#34;&#34;&#34;
        Chemical features (hydropathy, volume, charge, polarity, acceptor/donor)
        &#34;&#34;&#34;

        features = np.array([_PMAP(x) for x in seq])
        return features

    def _sse(self, chain_dict, seq):
        &#34;&#34;&#34;
        Secondary structure features
        &#34;&#34;&#34;

        sse_map = {&#34;c&#34;: [0, 0, 1], &#34;b&#34;: [0, 1, 0], &#34;a&#34;: [1, 0, 0], &#34;&#34;: [0, 0, 0]}
        sse = _annotate_sse(chain_dict[&#34;crd_bb&#34;])
        sse = np.array([sse_map[x] for x in sse]) * chain_dict[&#34;msk&#34;][:, None]
        return sse

    def _sidechain_coords(self, chain_dict, seq):
        &#34;&#34;&#34;
        Sidechain coordinates
        &#34;&#34;&#34;

        crd_sc = chain_dict[&#34;crd_sc&#34;]
        return crd_sc

    def _process(self, filename, rewrite=False, max_length=None):
        &#34;&#34;&#34;
        Process a proteinflow file and save it as ProteinMPNN features
        &#34;&#34;&#34;

        input_file = os.path.join(self.dataset_folder, filename)
        no_extension_name = filename.split(&#34;.&#34;)[0]
        try:
            with open(input_file, &#34;rb&#34;) as f:
                data = pickle.load(f)
        except:
            print(f&#34;{input_file=}&#34;)
        chains = sorted(data.keys())
        if self.entry_type == &#34;biounit&#34;:
            chain_sets = [chains]
        elif self.entry_type == &#34;chain&#34;:
            chain_sets = [[x] for x in chains]
        elif self.entry_type == &#34;pair&#34;:
            chain_sets = list(combinations(chains, 2))
        else:
            raise RuntimeError(
                &#34;Unknown entry type, please choose from [&#39;biounit&#39;, &#39;chain&#39;, &#39;pair&#39;]&#34;
            )
        output_names = []
        for chains_i, chain_set in enumerate(chain_sets):
            output_file = os.path.join(
                self.features_folder, no_extension_name + f&#34;_{chains_i}.pickle&#34;
            )
            pass_set = False
            add_name = True
            if os.path.exists(output_file) and not rewrite:
                pass_set = True
                if max_length is not None:
                    if sum([len(data[x][&#34;seq&#34;]) for x in chain_set]) &gt; max_length:
                        add_name = False
            else:
                X = []
                S = []
                mask = []
                mask_original = []
                chain_encoding_all = []
                residue_idx = []
                node_features = defaultdict(lambda: [])
                last_idx = 0
                chain_dict = {}

                if max_length is not None:
                    if sum([len(data[x][&#34;seq&#34;]) for x in chain_set]) &gt; max_length:
                        pass_set = True
                        add_name = False

                if self.entry_type == &#34;pair&#34;:
                    intersect = []
                    X1 = data[chain_set[0]][&#34;crd_bb&#34;][
                        data[chain_set[0]][&#34;msk&#34;].astype(bool)
                    ]
                    X2 = data[chain_set[1]][&#34;crd_bb&#34;][
                        data[chain_set[1]][&#34;msk&#34;].astype(bool)
                    ]
                    for dim in range(3):
                        min_dim_1 = X1[:, :, dim].min()
                        max_dim_1 = X1[:, :, dim].max()
                        min_dim_2 = X2[:, :, dim].min()
                        max_dim_2 = X2[:, :, dim].max()
                        if min_dim_1 - 4 &lt;= max_dim_2 and max_dim_1 &gt;= min_dim_2 - 4:
                            intersect.append(True)
                        else:
                            intersect.append(False)
                            break
                    if not all(intersect):
                        pass_set = True
                        add_name = False
            if add_name:
                output_names.append(
                    (os.path.basename(no_extension_name), output_file, chain_set)
                )
            if pass_set:
                continue

            for chain_i, chain in enumerate(chain_set):
                seq = torch.tensor([self.alphabet_dict[x] for x in data[chain][&#34;seq&#34;]])
                S.append(seq)
                mask_original.append(deepcopy(data[chain][&#34;msk&#34;]))
                if self.interpolate != &#34;none&#34;:
                    data[chain][&#34;crd_bb&#34;], data[chain][&#34;msk&#34;] = self._interpolate(
                        data[chain][&#34;crd_bb&#34;], data[chain][&#34;msk&#34;]
                    )
                X.append(data[chain][&#34;crd_bb&#34;])
                mask.append(data[chain][&#34;msk&#34;])
                residue_idx.append(torch.arange(len(data[chain][&#34;seq&#34;])) + last_idx)
                last_idx = residue_idx[-1][-1] + 100
                chain_encoding_all.append(torch.ones(len(data[chain][&#34;seq&#34;])) * chain_i)
                chain_dict[chain] = chain_i
                for name in self.feature_types:
                    if name not in self.feature_functions:
                        continue
                    func = self.feature_functions[name]
                    node_features[name].append(func(data[chain], seq))

            out = {}
            out[&#34;X&#34;] = torch.from_numpy(np.concatenate(X, 0))
            out[&#34;S&#34;] = torch.cat(S)
            out[&#34;mask&#34;] = torch.from_numpy(np.concatenate(mask))
            out[&#34;mask_original&#34;] = torch.from_numpy(np.concatenate(mask_original))
            out[&#34;chain_encoding_all&#34;] = torch.cat(chain_encoding_all)
            out[&#34;residue_idx&#34;] = torch.cat(residue_idx)
            out[&#34;chain_dict&#34;] = chain_dict
            for key, value_list in node_features.items():
                out[key] = torch.from_numpy(np.concatenate(value_list))
            with open(output_file, &#34;wb&#34;) as f:
                pickle.dump(out, f)
        return output_names

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        chain_id = None
        if self.clusters is None:
            id = self.data[idx]  # data is already filtered by length
            chain_id = random.choice(list(self.files[id].keys()))
        else:
            cluster = self.data[idx]
            id = None
            chain_n = -1
            # print(f&#39;{self.clusters[cluster]=}&#39;)
            while (
                id is None or len(self.files[id][chain_id]) == 0
            ):  # some IDs can be filtered out by length
                if self.shuffle_clusters:
                    chain_n = random.randint(0, len(self.clusters[cluster]) - 1)
                else:
                    chain_n += 1
                id, chain_id = self.clusters[cluster][
                    chain_n
                ]  # get id and chain from cluster
                # print(f&#39;{id=}, {len(self.files[id][chain_id])=}&#39;)
        file = random.choice(self.files[id][chain_id])
        if self.loaded is None:
            with open(file, &#34;rb&#34;) as f:
                data = pickle.load(f)
        else:
            data = deepcopy(self.loaded[file])
        data[&#34;chain_id&#34;] = data[&#34;chain_dict&#34;][chain_id]
        data.pop(&#34;chain_dict&#34;)
        return data</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
</dd>
<dt id="proteinflow.ProteinLoader"><code class="flex name class">
<span>class <span class="ident">ProteinLoader</span></span>
<span>(</span><span>dataset, lower_limit=15, upper_limit=100, mask_residues=True, mask_whole_chains=False, mask_frac=None, force_binding_sites_frac=0, shuffle_batches=True, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A subclass of <code>torch.data.utils.DataLoader</code> tuned for the <code><a title="proteinflow" href="#proteinflow">proteinflow</a></code> dataset</p>
<p>Creates and iterates over an instance of <code><a title="proteinflow.ProteinDataset" href="#proteinflow.ProteinDataset">ProteinDataset</a></code>, omitting the <code>'chain_dict'</code> keys.
See the <code><a title="proteinflow.ProteinDataset" href="#proteinflow.ProteinDataset">ProteinDataset</a></code> docs for more information.</p>
<p>If batch size is larger than one, all objects are padded with zeros at the ends to reach the length of the
longest protein in the batch.</p>
<p>If <code>mask_residues</code> is <code>True</code>, an additional <code>'masked_res'</code> key is added to the output. The value is a binary
tensor shaped <code>(B, L)</code> where 1 denotes the part that needs to be predicted and 0 is everything else. The tensors are generated
according to the following rules:
- if <code>mask_whole_chains</code> is <code>True</code>, the whole chain is masked
- if <code>mask_frac</code> is given, the number of residues to mask is <code>mask_frac</code> times the length of the chain,
- otherwise, the number of residues to mask is sampled uniformly from the range [<code>lower_limit</code>, <code>upper_limit</code>].</p>
<p>If <code>force_binding_sites_frac</code> &gt; 0 and <code>mask_whole_chains</code> is <code>False</code>, in the fraction of cases where a chain
from a polymer is sampled, the center of the masked region will be forced to be in a binding site.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code><a title="proteinflow.ProteinDataset" href="#proteinflow.ProteinDataset">ProteinDataset</a></code></dt>
<dd>a ProteinDataset instance</dd>
<dt><strong><code>lower_limit</code></strong> :&ensp;<code>int</code>, default <code>15</code></dt>
<dd>the minimum number of residues to mask</dd>
<dt><strong><code>upper_limit</code></strong> :&ensp;<code>int</code>, default <code>100</code></dt>
<dd>the maximum number of residues to mask</dd>
<dt><strong><code>mask_frac</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>if given, the <code>lower_limit</code> and <code>upper_limit</code> are ignored and the number of residues to mask is <code>mask_frac</code> times the length of the chain</dd>
<dt><strong><code>mask_whole_chains</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, <code>upper_limit</code>, <code>force_binding_sites</code> and <code>lower_limit</code> are ignored and the whole chain is masked instead</dd>
<dt><strong><code>force_binding_sites_frac</code></strong> :&ensp;<code>float</code>, default <code>0</code></dt>
<dd>if &gt; 0, in the fraction of cases where a chain from a polymer is sampled, the center of the masked region will be
forced to be in a binding site</dd>
<dt><strong><code>shuffle_clusters</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>if <code>True</code>, a new representative is randomly selected for each cluster at each epoch (if <code>clustering_dict_path</code> is given)</dd>
<dt><strong><code>shuffle_batches</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>if <code>True</code>, the batches are shuffled at each epoch</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ProteinLoader(DataLoader):
    &#34;&#34;&#34;
    A subclass of `torch.data.utils.DataLoader` tuned for the `proteinflow` dataset

    Creates and iterates over an instance of `ProteinDataset`, omitting the `&#39;chain_dict&#39;` keys.
    See the `ProteinDataset` docs for more information.

    If batch size is larger than one, all objects are padded with zeros at the ends to reach the length of the
    longest protein in the batch.

    If `mask_residues` is `True`, an additional `&#39;masked_res&#39;` key is added to the output. The value is a binary
    tensor shaped `(B, L)` where 1 denotes the part that needs to be predicted and 0 is everything else. The tensors are generated
    according to the following rules:
    - if `mask_whole_chains` is `True`, the whole chain is masked
    - if `mask_frac` is given, the number of residues to mask is `mask_frac` times the length of the chain,
    - otherwise, the number of residues to mask is sampled uniformly from the range [`lower_limit`, `upper_limit`].

    If `force_binding_sites_frac` &gt; 0 and `mask_whole_chains` is `False`, in the fraction of cases where a chain
    from a polymer is sampled, the center of the masked region will be forced to be in a binding site.
    &#34;&#34;&#34;

    def __init__(
        self,
        dataset,
        lower_limit=15,
        upper_limit=100,
        mask_residues=True,
        mask_whole_chains=False,
        mask_frac=None,
        force_binding_sites_frac=0,
        shuffle_batches=True,
        *args,
        **kwargs,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        dataset : ProteinDataset
            a ProteinDataset instance
        lower_limit : int, default 15
            the minimum number of residues to mask
        upper_limit : int, default 100
            the maximum number of residues to mask
        mask_frac : float, optional
            if given, the `lower_limit` and `upper_limit` are ignored and the number of residues to mask is `mask_frac` times the length of the chain
        mask_whole_chains : bool, default False
            if `True`, `upper_limit`, `force_binding_sites` and `lower_limit` are ignored and the whole chain is masked instead
        force_binding_sites_frac : float, default 0
            if &gt; 0, in the fraction of cases where a chain from a polymer is sampled, the center of the masked region will be
            forced to be in a binding site
        shuffle_clusters : bool, default True
            if `True`, a new representative is randomly selected for each cluster at each epoch (if `clustering_dict_path` is given)
        shuffle_batches : bool, default True
            if `True`, the batches are shuffled at each epoch
        &#34;&#34;&#34;

        super().__init__(
            dataset,
            collate_fn=_PadCollate(
                mask_residues=mask_residues,
                mask_whole_chains=mask_whole_chains,
                mask_frac=mask_frac,
                lower_limit=lower_limit,
                upper_limit=upper_limit,
                force_binding_sites_frac=force_binding_sites_frac,
            ),
            shuffle=shuffle_batches,
            *args,
            **kwargs,
        )

    @staticmethod
    def from_args(
        dataset_folder,
        features_folder=&#34;./data/tmp/&#34;,
        clustering_dict_path=None,
        max_length=None,
        rewrite=False,
        use_fraction=1,
        load_to_ram=False,
        debug=False,
        interpolate=&#34;none&#34;,
        node_features_type=None,
        entry_type=&#34;biounit&#34;,  # biounit, chain, pair
        classes_to_exclude=None,
        lower_limit=15,
        upper_limit=100,
        mask_residues=True,
        mask_whole_chains=False,
        mask_frac=None,
        force_binding_sites_frac=0,
        shuffle_clusters=True,
        shuffle_batches=True,
        *args,
        **kwargs,
    ) -&gt; None:
        &#34;&#34;&#34;
        Creates a `ProteinLoader` instance with a `ProteinDataset` from the given arguments

        Parameters
        ----------
        dataset_folder : str
            the path to the folder with proteinflow format input files (assumes that files are named {biounit_id}.pickle)
        features_folder : str
            the path to the folder where the ProteinMPNN features will be saved
        clustering_dict_path : str, optional
            path to the pickled clustering dictionary (keys are cluster ids, values are (biounit id, chain id) tuples)
        max_length : int, optional
            entries with total length of chains larger than `max_length` will be disregarded
        rewrite : bool, default False
            if `False`, existing feature files are not overwritten
        use_fraction : float, default 1
            the fraction of the clusters to use (first N in alphabetic order)
        load_to_ram : bool, default False
            if `True`, the data will be stored in RAM (use with caution! if RAM isn&#39;t big enough the machine might crash)
        debug : bool, default False
            only process 1000 files
        interpolate : {&#34;none&#34;, &#34;only_middle&#34;, &#34;all&#34;}
            `&#34;none&#34;` for no interpolation, `&#34;only_middle&#34;` for only linear interpolation in the middle, `&#34;all&#34;` for linear interpolation + ends generation
        node_features_type : {&#34;dihedral&#34;, &#34;sidechain_orientation&#34;, &#34;chemical&#34;, &#34;secondary_structure&#34;, &#34;sidechain_coords&#34;, or combinations with &#34;+&#34;}, optional
            the type of node features, e.g. `&#34;dihedral&#34;` or `&#34;sidechain_orientation+chemical&#34;`
        entry_type : {&#34;biounit&#34;, &#34;chain&#34;, &#34;pair&#34;}
            the type of entries to generate (`&#34;biounit&#34;` for biounit-level, `&#34;chain&#34;` for chain-level, `&#34;pair&#34;` for chain-chain pairs)
        classes_to_exclude : list of str, optional
            a list of classes to exclude from the dataset (select from `&#34;single_chains&#34;`, `&#34;heteromers&#34;`, `&#34;homomers&#34;`)
        lower_limit : int, default 15
            the minimum number of residues to mask
        upper_limit : int, default 100
            the maximum number of residues to mask
        mask_frac : float, optional
            if given, the `lower_limit` and `upper_limit` are ignored and the number of residues to mask is `mask_frac` times the length of the chain
        mask_whole_chains : bool, default False
            if `True`, `upper_limit`, `force_binding_sites` and `lower_limit` are ignored and the whole chain is masked instead
        force_binding_sites_frac : float, default 0
            if &gt; 0, in the fraction of cases where a chain from a polymer is sampled, the center of the masked region will be
            forced to be in a binding site
        shuffle_clusters : bool, default True
            if `True`, a new representative is randomly selected for each cluster at each epoch (if `clustering_dict_path` is given)
        shuffle_batches : bool, default True
            if `True`, the batches are shuffled at each epoch
        &#34;&#34;&#34;

        dataset = ProteinDataset(
            dataset_folder=dataset_folder,
            features_folder=features_folder,
            clustering_dict_path=clustering_dict_path,
            max_length=max_length,
            rewrite=rewrite,
            use_fraction=use_fraction,
            load_to_ram=load_to_ram,
            debug=debug,
            interpolate=interpolate,
            node_features_type=node_features_type,
            entry_type=entry_type,
            classes_to_exclude=classes_to_exclude,
            shuffle_clusters=shuffle_clusters,
        )
        return ProteinLoader(
            dataset=dataset,
            lower_limit=lower_limit,
            upper_limit=upper_limit,
            mask_residues=mask_residues,
            mask_whole_chains=mask_whole_chains,
            mask_frac=mask_frac,
            force_binding_sites_frac=force_binding_sites_frac,
            shuffle_batches=shuffle_batches,
            *args,
            **kwargs,
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataloader.DataLoader</li>
<li>typing.Generic</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="proteinflow.ProteinLoader.batch_size"><code class="name">var <span class="ident">batch_size</span> :Â Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="proteinflow.ProteinLoader.dataset"><code class="name">var <span class="ident">dataset</span> :Â torch.utils.data.dataset.Dataset[+T_co]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="proteinflow.ProteinLoader.drop_last"><code class="name">var <span class="ident">drop_last</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="proteinflow.ProteinLoader.num_workers"><code class="name">var <span class="ident">num_workers</span> :Â int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="proteinflow.ProteinLoader.pin_memory"><code class="name">var <span class="ident">pin_memory</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="proteinflow.ProteinLoader.pin_memory_device"><code class="name">var <span class="ident">pin_memory_device</span> :Â str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="proteinflow.ProteinLoader.prefetch_factor"><code class="name">var <span class="ident">prefetch_factor</span> :Â Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="proteinflow.ProteinLoader.sampler"><code class="name">var <span class="ident">sampler</span> :Â Union[torch.utils.data.sampler.Sampler,Â Iterable]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="proteinflow.ProteinLoader.timeout"><code class="name">var <span class="ident">timeout</span> :Â float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="proteinflow.ProteinLoader.from_args"><code class="name flex">
<span>def <span class="ident">from_args</span></span>(<span>dataset_folder, features_folder='./data/tmp/', clustering_dict_path=None, max_length=None, rewrite=False, use_fraction=1, load_to_ram=False, debug=False, interpolate='none', node_features_type=None, entry_type='biounit', classes_to_exclude=None, lower_limit=15, upper_limit=100, mask_residues=True, mask_whole_chains=False, mask_frac=None, force_binding_sites_frac=0, shuffle_clusters=True, shuffle_batches=True, *args, **kwargs) â€‘>Â None</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="proteinflow.ProteinLoader" href="#proteinflow.ProteinLoader">ProteinLoader</a></code> instance with a <code><a title="proteinflow.ProteinDataset" href="#proteinflow.ProteinDataset">ProteinDataset</a></code> from the given arguments</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>the path to the folder with proteinflow format input files (assumes that files are named {biounit_id}.pickle)</dd>
<dt><strong><code>features_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>the path to the folder where the ProteinMPNN features will be saved</dd>
<dt><strong><code>clustering_dict_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>path to the pickled clustering dictionary (keys are cluster ids, values are (biounit id, chain id) tuples)</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>entries with total length of chains larger than <code>max_length</code> will be disregarded</dd>
<dt><strong><code>rewrite</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>False</code>, existing feature files are not overwritten</dd>
<dt><strong><code>use_fraction</code></strong> :&ensp;<code>float</code>, default <code>1</code></dt>
<dd>the fraction of the clusters to use (first N in alphabetic order)</dd>
<dt><strong><code>load_to_ram</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, the data will be stored in RAM (use with caution! if RAM isn't big enough the machine might crash)</dd>
<dt><strong><code>debug</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>only process 1000 files</dd>
<dt><strong><code>interpolate</code></strong> :&ensp;<code>{"none", "only_middle", "all"}</code></dt>
<dd><code>"none"</code> for no interpolation, <code>"only_middle"</code> for only linear interpolation in the middle, <code>"all"</code> for linear interpolation + ends generation</dd>
<dt><strong><code>node_features_type</code></strong> :&ensp;<code>{"dihedral", "sidechain_orientation", "chemical", "secondary_structure", "sidechain_coords",</code> or <code>combinations with "+"}</code>, optional</dt>
<dd>the type of node features, e.g. <code>"dihedral"</code> or <code>"sidechain_orientation+chemical"</code></dd>
<dt><strong><code>entry_type</code></strong> :&ensp;<code>{"biounit", "chain", "pair"}</code></dt>
<dd>the type of entries to generate (<code>"biounit"</code> for biounit-level, <code>"chain"</code> for chain-level, <code>"pair"</code> for chain-chain pairs)</dd>
<dt><strong><code>classes_to_exclude</code></strong> :&ensp;<code>list</code> of <code>str</code>, optional</dt>
<dd>a list of classes to exclude from the dataset (select from <code>"single_chains"</code>, <code>"heteromers"</code>, <code>"homomers"</code>)</dd>
<dt><strong><code>lower_limit</code></strong> :&ensp;<code>int</code>, default <code>15</code></dt>
<dd>the minimum number of residues to mask</dd>
<dt><strong><code>upper_limit</code></strong> :&ensp;<code>int</code>, default <code>100</code></dt>
<dd>the maximum number of residues to mask</dd>
<dt><strong><code>mask_frac</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>if given, the <code>lower_limit</code> and <code>upper_limit</code> are ignored and the number of residues to mask is <code>mask_frac</code> times the length of the chain</dd>
<dt><strong><code>mask_whole_chains</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, <code>upper_limit</code>, <code>force_binding_sites</code> and <code>lower_limit</code> are ignored and the whole chain is masked instead</dd>
<dt><strong><code>force_binding_sites_frac</code></strong> :&ensp;<code>float</code>, default <code>0</code></dt>
<dd>if &gt; 0, in the fraction of cases where a chain from a polymer is sampled, the center of the masked region will be
forced to be in a binding site</dd>
<dt><strong><code>shuffle_clusters</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>if <code>True</code>, a new representative is randomly selected for each cluster at each epoch (if <code>clustering_dict_path</code> is given)</dd>
<dt><strong><code>shuffle_batches</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>if <code>True</code>, the batches are shuffled at each epoch</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_args(
    dataset_folder,
    features_folder=&#34;./data/tmp/&#34;,
    clustering_dict_path=None,
    max_length=None,
    rewrite=False,
    use_fraction=1,
    load_to_ram=False,
    debug=False,
    interpolate=&#34;none&#34;,
    node_features_type=None,
    entry_type=&#34;biounit&#34;,  # biounit, chain, pair
    classes_to_exclude=None,
    lower_limit=15,
    upper_limit=100,
    mask_residues=True,
    mask_whole_chains=False,
    mask_frac=None,
    force_binding_sites_frac=0,
    shuffle_clusters=True,
    shuffle_batches=True,
    *args,
    **kwargs,
) -&gt; None:
    &#34;&#34;&#34;
    Creates a `ProteinLoader` instance with a `ProteinDataset` from the given arguments

    Parameters
    ----------
    dataset_folder : str
        the path to the folder with proteinflow format input files (assumes that files are named {biounit_id}.pickle)
    features_folder : str
        the path to the folder where the ProteinMPNN features will be saved
    clustering_dict_path : str, optional
        path to the pickled clustering dictionary (keys are cluster ids, values are (biounit id, chain id) tuples)
    max_length : int, optional
        entries with total length of chains larger than `max_length` will be disregarded
    rewrite : bool, default False
        if `False`, existing feature files are not overwritten
    use_fraction : float, default 1
        the fraction of the clusters to use (first N in alphabetic order)
    load_to_ram : bool, default False
        if `True`, the data will be stored in RAM (use with caution! if RAM isn&#39;t big enough the machine might crash)
    debug : bool, default False
        only process 1000 files
    interpolate : {&#34;none&#34;, &#34;only_middle&#34;, &#34;all&#34;}
        `&#34;none&#34;` for no interpolation, `&#34;only_middle&#34;` for only linear interpolation in the middle, `&#34;all&#34;` for linear interpolation + ends generation
    node_features_type : {&#34;dihedral&#34;, &#34;sidechain_orientation&#34;, &#34;chemical&#34;, &#34;secondary_structure&#34;, &#34;sidechain_coords&#34;, or combinations with &#34;+&#34;}, optional
        the type of node features, e.g. `&#34;dihedral&#34;` or `&#34;sidechain_orientation+chemical&#34;`
    entry_type : {&#34;biounit&#34;, &#34;chain&#34;, &#34;pair&#34;}
        the type of entries to generate (`&#34;biounit&#34;` for biounit-level, `&#34;chain&#34;` for chain-level, `&#34;pair&#34;` for chain-chain pairs)
    classes_to_exclude : list of str, optional
        a list of classes to exclude from the dataset (select from `&#34;single_chains&#34;`, `&#34;heteromers&#34;`, `&#34;homomers&#34;`)
    lower_limit : int, default 15
        the minimum number of residues to mask
    upper_limit : int, default 100
        the maximum number of residues to mask
    mask_frac : float, optional
        if given, the `lower_limit` and `upper_limit` are ignored and the number of residues to mask is `mask_frac` times the length of the chain
    mask_whole_chains : bool, default False
        if `True`, `upper_limit`, `force_binding_sites` and `lower_limit` are ignored and the whole chain is masked instead
    force_binding_sites_frac : float, default 0
        if &gt; 0, in the fraction of cases where a chain from a polymer is sampled, the center of the masked region will be
        forced to be in a binding site
    shuffle_clusters : bool, default True
        if `True`, a new representative is randomly selected for each cluster at each epoch (if `clustering_dict_path` is given)
    shuffle_batches : bool, default True
        if `True`, the batches are shuffled at each epoch
    &#34;&#34;&#34;

    dataset = ProteinDataset(
        dataset_folder=dataset_folder,
        features_folder=features_folder,
        clustering_dict_path=clustering_dict_path,
        max_length=max_length,
        rewrite=rewrite,
        use_fraction=use_fraction,
        load_to_ram=load_to_ram,
        debug=debug,
        interpolate=interpolate,
        node_features_type=node_features_type,
        entry_type=entry_type,
        classes_to_exclude=classes_to_exclude,
        shuffle_clusters=shuffle_clusters,
    )
    return ProteinLoader(
        dataset=dataset,
        lower_limit=lower_limit,
        upper_limit=upper_limit,
        mask_residues=mask_residues,
        mask_whole_chains=mask_whole_chains,
        mask_frac=mask_frac,
        force_binding_sites_frac=force_binding_sites_frac,
        shuffle_batches=shuffle_batches,
        *args,
        **kwargs,
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a><ul>
<li><a href="#downloading-pre-computed-datasets">Downloading pre-computed datasets</a></li>
<li><a href="#running-the-pipeline">Running the pipeline</a></li>
<li><a href="#splitting">Splitting</a></li>
<li><a href="#using-the-data">Using the data</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="proteinflow.check_download_tags" href="#proteinflow.check_download_tags">check_download_tags</a></code></li>
<li><code><a title="proteinflow.check_pdb_snapshots" href="#proteinflow.check_pdb_snapshots">check_pdb_snapshots</a></code></li>
<li><code><a title="proteinflow.download_data" href="#proteinflow.download_data">download_data</a></code></li>
<li><code><a title="proteinflow.generate_data" href="#proteinflow.generate_data">generate_data</a></code></li>
<li><code><a title="proteinflow.get_error_summary" href="#proteinflow.get_error_summary">get_error_summary</a></code></li>
<li><code><a title="proteinflow.sidechain_order" href="#proteinflow.sidechain_order">sidechain_order</a></code></li>
<li><code><a title="proteinflow.split_data" href="#proteinflow.split_data">split_data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="proteinflow.ProteinDataset" href="#proteinflow.ProteinDataset">ProteinDataset</a></code></h4>
</li>
<li>
<h4><code><a title="proteinflow.ProteinLoader" href="#proteinflow.ProteinLoader">ProteinLoader</a></code></h4>
<ul class="two-column">
<li><code><a title="proteinflow.ProteinLoader.batch_size" href="#proteinflow.ProteinLoader.batch_size">batch_size</a></code></li>
<li><code><a title="proteinflow.ProteinLoader.dataset" href="#proteinflow.ProteinLoader.dataset">dataset</a></code></li>
<li><code><a title="proteinflow.ProteinLoader.drop_last" href="#proteinflow.ProteinLoader.drop_last">drop_last</a></code></li>
<li><code><a title="proteinflow.ProteinLoader.from_args" href="#proteinflow.ProteinLoader.from_args">from_args</a></code></li>
<li><code><a title="proteinflow.ProteinLoader.num_workers" href="#proteinflow.ProteinLoader.num_workers">num_workers</a></code></li>
<li><code><a title="proteinflow.ProteinLoader.pin_memory" href="#proteinflow.ProteinLoader.pin_memory">pin_memory</a></code></li>
<li><code><a title="proteinflow.ProteinLoader.pin_memory_device" href="#proteinflow.ProteinLoader.pin_memory_device">pin_memory_device</a></code></li>
<li><code><a title="proteinflow.ProteinLoader.prefetch_factor" href="#proteinflow.ProteinLoader.prefetch_factor">prefetch_factor</a></code></li>
<li><code><a title="proteinflow.ProteinLoader.sampler" href="#proteinflow.ProteinLoader.sampler">sampler</a></code></li>
<li><code><a title="proteinflow.ProteinLoader.timeout" href="#proteinflow.ProteinLoader.timeout">timeout</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>